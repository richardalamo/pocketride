{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Route Optimization with Chicago Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rf3hSrs_IdmQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns  \n",
    "import folium  \n",
    "from folium import plugins\n",
    "from folium import Choropleth, Circle, Marker\n",
    "#from folium.plugins import TimeSliderChoropleth\n",
    "import plotly.express as px\n",
    "import json\n",
    "import osmnx as ox  \n",
    "import networkx as nx \n",
    "import geopandas as gpd\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "from sklearn.cluster import KMeans \n",
    "from scikitplot.cluster import plot_elbow_curve\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import gc\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error, r2_score, make_scorer\n",
    "from skopt import BayesSearchCV\n",
    "from prophet import Prophet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [Chapter 1 Match Real Trip Routes with Calculated Shortest Routes from OSMnx](#Chapter-1-Match-Real-Trip-Routes-with-Calculated-Shortest-Routes-from-OSMnx)\n",
    "  - [1.1 Data preparation and OSMnx graph creation](#1.1-Data-preparation-and-OSMnx-graph-creation)\n",
    "  - [1.2 Match the end points of each trip with the nodes from the OSMnx graph and calculate the shortest route with the obtained nodes](#1.2-Match-the-end-points-of-each-trip-with-the-nodes-from-the-OSMnx-graph-and-calculate-the-shortest-route-with-the-obtained-nodes)\n",
    "  - [1.3 Filter trips based on the match result](#1.3-Filter-trips-based-on-the-match-result)\n",
    "    - [1.3.1 Filter trips by the point-to-nearest-node distance](#1.3.1-Filter-trips-by-the-point-to-nearest-node-distance)\n",
    "    - [1.3.2 Filter trips by the absolute difference between the real trip miles and the calculated shortest route length](#1.3.2-Filter-trips-by-the-absolute-difference-between-the-real-trip-miles-and-the-calculated-shortest-route-length)\n",
    "      - [1.3.2.1 compare the trip features before and after filter with different thresholds](#1.3.2.1-compare-the-trip-features-before-and-after-filter-with-different-thresholds)\n",
    "      - [1.3.2.2 compare the temporal traffic pattern before and after filter with the 500m threshold](#1.3.2.2-compare-the-temporal-traffic-pattern-before-and-after-filter-with-the-500m-threshold)    \n",
    "- [Chapter 2 Create Segment Level Dataset](#Chapter-2-Create-Segment-Level-Dataset)\n",
    "  - [2.1 Cluster all road segments of Chicago into regions](#2.1-Cluster-all-road-segments-of-Chicago-into-regions)\n",
    "  - [2.2 Extract the road segments details from G](#2.2-Extract-the-road-segments-details-from-G)\n",
    "  - [2.3 Display the road segments in our dataset on OSMnx graph and check their coverage ratio in each region](#2.3-Display-the-road-segments-in-our-dataset-on-OSMnx-graph-and-check-their-coverage-ratio-in-each-region)\n",
    "  - [2.4 Complete the segment level traffic information](#2.4-Complete-the-segment-level-traffic-information)\n",
    "  \n",
    "- [Chapter 3 Check the Temporal Traffic Pattern Completeness for Each Region](#Chapter-3-Check-the-Temporal-Traffic-Pattern-Completeness-for-Each-Region)\n",
    "- [Chapter 4 Road Segments Clustering within Each Region](#Chapter-4-Road-Segments-Clustering-within-Each-Region)\n",
    "  - [4.1 Data preparation for KMeans clustering](#4.1-Data-preparation-for-KMeans-clustering)\n",
    "  - [4.2 Use elbow method to determine K for each region](#4.2-Use-elbow-method-to-determine-K-for-each-region)\n",
    "  - [4.3 Save the obtained cluster information for each region to the segment level dataframe](#4.3-Save-the-obtained-cluster-information-for-each-region-to-the-segment-level-dataframe)\n",
    "  - [4.4 Display the above plotted results onto map](#4.4-Display-the-above-plotted-results-onto-map)\n",
    "    - [4.4.1 Region data preparation](#4.4.1-Region-data-preparation)\n",
    "    - [4.4.2 Plot road segments on map, colored by congestion level](#4.4.2-Plot-road-segments-on-map,-colored-by-congestion-level)\n",
    "- [Chapter 5  Build ML models for predicting the  travel time for each road segment cluster](#Chapter-5--Build-ML-models-for-predicting-the--travel-time-for-each-road-segment-cluster)\n",
    "  - [5.1 Check the clustering accuracy by plotting the inferred travel time histogram for each cluster](#5.1-Check-the-clustering-accuracy-by-plotting-the-inferred-travel-time-histogram-for-each-cluster)\n",
    "  - [5.2 EDA for feature engineering](#5.2-EDA-for-feature-engineering)\n",
    "    - [5.2.1 fill up missing hours](#5.2.1-fill-up-missing-hours)\n",
    "    - [5.2.2 Generate lag and rolling features](#5.2.2-Generate-lag-and-rolling-features)\n",
    "    - [5.2.3 EDA on individual features and correlation between features](#5.2.3-EDA-on-individual-features-and-correlation-between-features)\n",
    "      - [5.2.3.1 Exploring single numerical features](#5.2.3.1-Exploring-single-numerical-features)\n",
    "      - [5.2.3.2 Exploring the correlation between features](#5.2.3.2-Exploring-the-correlation-between-features)\n",
    "      - [5.2.3.3 Use Xgboost as the base model to do feature engneering](#5.2.3.3-Use-Xgboost-as-the-base-model-to-do-feature-engneering)\n",
    "   - [5.3 Machine Learning Modeling](#5.3-Machine-Learning-Modeling)\n",
    "     - [5.3.1 Xgboost model with Bayesian optimization (BayesSearchCV)](#5.3.1-Xgboost-model-with-Bayesian-optimization-(BayesSearchCV))\n",
    "     - [5.3.2 Prophet model](#5.3.2-Prophet-model)\n",
    "   - [5.4 Use the prediction results to do route optimization and compare the results with that based on the inferred travel time](#5.4-Use-the-prediction-results-to-do-route-optimization-and-compare-the-results-with-that-based-on-the-inferred-travel-time)\n",
    "     - [5.4.1 Merge the prediction results for each cluster to the original regional dataset](#5.4.1-Merge-the-prediction-results-for-each-cluster-to-the-original-regional-dataset)\n",
    "     - [5.4.2 Compare the route optimization results by using the predition results on cluster level and the inferred travel time on segment level](#5.4.2-Compare-the-route-optimization-results-by-using-the-predition-results-on-cluster-level-and-the-inferred-travel-time-on-segment-level)\n",
    "       - [5.4.2.1 Analyze the difference between the inferred_travel_time_sec and the predicted travel time for each segment](#5.4.2.1-Analyze-the-difference-between-the-inferred_travel_time_sec-and-the-predicted-travel-time-for-each-segment)\n",
    "       - [5.4.2.2 Compare the route optimization results by the prediction and the inferred travel time](#5.4.2.2-Compare-the-route-optimization-results-by-the-prediction-and-the-inferred-travel-time)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1 Match Real Trip Routes with Calculated Shortest Routes from OSMnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9k-uzzw9Vn73"
   },
   "source": [
    "### 1.1 Data preparation and OSMnx graph creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v78RtP1gNojV"
   },
   "outputs": [],
   "source": [
    "# import chicago data\n",
    "df = pd.read_csv('chicago_data_5.2_perc_sample.csv')\n",
    "# drop na values in pickup_centroid_location or dropoff_centroid_location\n",
    "data=df.dropna(subset=['pickup_centroid_location', 'dropoff_centroid_location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an OSMnx graph for Chicago city\n",
    "place = 'Chicago, Illinois, USA'\n",
    "G = ox.graph_from_place(place, network_type='drive')\n",
    "\n",
    "#add speed limit and free flow travel_time attributes to the edges.\n",
    "G = ox.add_edge_speeds(G)\n",
    "G = ox.add_edge_travel_times(G)\n",
    "# create geo-dataframe for both nodes and edges \n",
    "nodes, edges = ox.graph_to_gdfs(G, nodes=True, edges=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Match the end points of each trip with the nodes from the OSMnx graph and calculate the shortest route with the obtained nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe data_group to only include the unique ('pickup_centroid_location', 'dropoff_centroid_location') \n",
    "data_group=data.drop_duplicates(['pickup_centroid_location', 'dropoff_centroid_location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the end points of each pair into nodes of the graph:\n",
    "data_group['start_node']=data_group.apply(lambda row: \n",
    "                                                        ox.distance.nearest_nodes(\n",
    "                                                            G, \n",
    "                                                            float(re.findall(pattern, row['pickup_centroid_location'])[0]),\n",
    "                                                            float(re.findall(pattern, row['pickup_centroid_location'])[1])),\n",
    "                                                            axis=1)\n",
    "data_group['end_node']=data_group.apply(lambda row: \n",
    "                                                        ox.distance.nearest_nodes(\n",
    "                                                            G, \n",
    "                                                            float(re.findall(pattern, row['dropoff_centroid_location'])[0]),\n",
    "                                                            float(re.findall(pattern, row['dropoff_centroid_location'])[1])),\n",
    "                                                            axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distance between the point and its nearest node\n",
    "data_group['start_node_distance'] = data_group.apply(lambda row: \n",
    "                                                                       ox.distance.great_circle_vec(\n",
    "                                                                       float(re.findall(pattern, row['pickup_centroid_location'])[1]),\n",
    "                                                                       float(re.findall(pattern, row['pickup_centroid_location'])[0]),\n",
    "                                                                       G.nodes[row['start_node']]['y'], G.nodes[row['start_node']]['x']),\n",
    "                                                                   axis=1)\n",
    "data_group['end_node_distance'] = data_group.apply(lambda row: \n",
    "                                                                       ox.distance.great_circle_vec(\n",
    "                                                                       float(re.findall(pattern, row['dropoff_centroid_location'])[1]),\n",
    "                                                                       float(re.findall(pattern, row['dropoff_centroid_location'])[0]),\n",
    "                                                                       G.nodes[row['end_node']]['y'], G.nodes[row['end_node']]['x']),\n",
    "                                                                   axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the shortest route by length for each pair of the obtained nodes \n",
    "def get_shortest_route(row):\n",
    "    try:\n",
    "        shortest_route=nx.shortest_path(G, row['start_node'], row['end_node'], weight='length')\n",
    "        shortest_route_length=nx.shortest_path_length(G, row['start_node'], row['end_node'], weight='length')   \n",
    "    except nx.NetworkXNoPath:\n",
    "        shortest_route=np.nan\n",
    "        shortest_route_length=np.nan\n",
    "    return shortest_route, shortest_route_length\n",
    "data_group[['shortest_route','shortest_route_length']]= data_group.apply(\n",
    "                                                                                                                                 lambda row:\n",
    "                                                                                                                                pd.Series(get_shortest_route(row)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the new generated columns in above with the original data dataframe and rename it as data_merged\n",
    "data_group_col=data_group[['pickup_centroid_location', 'dropoff_centroid_location', 'start_node','end_node',\n",
    "                           'start_node_distance', 'end_node_distance','shortest_route', 'shortest_route_length']]\n",
    "data_merged=pd.merge(data, data_group_col, on=['pickup_centroid_location', 'dropoff_centroid_location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this dataframe for an easier access in the future\n",
    "data_merged.to_csv('Chicago_orig.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged=pd.read_csv('Chicago_orig.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Filter trips based on the match result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Filter trips by the point-to-nearest-node distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the statistics info of the distance between the pickup point and the found nearest node\n",
    "data_merged['start_node_distance'].describe().drop('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the statistics info of the distance between the dropoff point and the found nearest node\n",
    "data_merged['end_node_distance'].describe().drop('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis: \n",
    "- the distance of ~82m between the closest node and the pickup/dropoff location corresponds to 75 percentile, we can start with  82m as the threshold to filter trips, and then try other thresholds based on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the retained data ratio by trying different thresholds\n",
    "data_ratio_thre_82m=len(data_merged[(data_merged['start_node_distance']<82) \n",
    "                         & (data_merged['end_node_distance']<82)])/len(data_merged)\n",
    "data_ratio_thre_100m=len(data_merged[(data_merged['start_node_distance']<100) & \n",
    "                (data_merged['end_node_distance']<100)])/len(data_merged)\n",
    "data_ratio_thre_150m=len(data_merged[(data_merged['start_node_distance']<150) \n",
    "                & (data_merged['end_node_distance']<150)])/len(data_merged)\n",
    "data_ratio_thre_200m=len(data_merged[(data_merged['start_node_distance']<200) \n",
    "                & (data_merged['end_node_distance']<200)])/len(data_merged)\n",
    "data_ratio_thre_500m=len(data_merged[(data_merged['start_node_distance']<500) \n",
    "                & (data_merged['end_node_distance']<500)])/len(data_merged)\n",
    "data_ratio_thre_1000m=len(data_merged[(data_merged['start_node_distance']<1000) \n",
    "                                     & (data_merged['end_node_distance']<1000)])/len(data_merged)\n",
    "print(f'data_ratio_thre_82m: {data_ratio_thre_82m},\\\n",
    "    data_ratio_thre_100m: {data_ratio_thre_100m},\\\n",
    "     data_ratio_thre_150m: {data_ratio_thre_150m},\\\n",
    "     data_ratio_thre_200m: {data_ratio_thre_200m},\\\n",
    "     data_ratio_thre_500m: {data_ratio_thre_500m},\\\n",
    "     data_ratio_thre_1000m: {data_ratio_thre_1000m}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "- from the above result, we can see that since the threshold of 150m, the increase of the retained data ratio becomes insignificant, and by considering that 150m distance is still reasonable, we can conclude that 150m should be the best threshod to filter trips so as to keep as many as possible trips reasonably. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # create a new dataframe called data_merged_nodes reflecting the filter by nodes as mentioned above. \n",
    "data_merged_nodes=data_merged[(data_merged['start_node_distance']<150) &\n",
    "                                                                     (data_merged['end_node_distance']<150)  &\n",
    "                                                                     (data_merged['start_node'] != data_merged['end_node'])\n",
    "                              ]\n",
    "# drop the invalid routes \n",
    "data_merged_nodes.dropna(subset=['shortest_route'], inplace=True)\n",
    "# add a new column about the absolute difference between the actual trip_miles and the calculated shortest route length\n",
    "data_merged_nodes['absdiff_trip_shortest_meters']=abs(data_merged_nodes['trip_miles']*1609.344-data_merged_nodes['shortest_route_length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Filter trips by the absolute difference between the real trip miles and the calculated shortest route length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the statistics info of the absolute difference between the real trip miles and the calcuated shortest route length\n",
    "data_merged_nodes['absdiff_trip_shortest_meters'].describe().drop('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the boxplot by focusing on the difference range within 2500 for a better visualization\n",
    "Y=data_merged_nodes[data_merged_nodes['absdiff_trip_shortest_meters']<2500]\n",
    "Y['absdiff_trip_shortest_meters'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "- it seems we can consider 1610 as the maximum threshold since it retains 75% data.\n",
    "- we can also try 1000m and 500m for comparison since both thresholds retain at least ~50% data and are more reasonable than 1610m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_merged_routes_diffXXX was obtained after filtering trips with the threshold of XXXm \n",
    "data_merged_routes_diff1610=data_merged_nodes[data_merged_nodes['absdiff_trip_shortest_meters']<1610]\n",
    "data_merged_routes_diff500=data_merged_nodes[data_merged_nodes['absdiff_trip_shortest_meters']<500]\n",
    "data_merged_routes_diff1000=data_merged_nodes[data_merged_nodes['absdiff_trip_shortest_meters']<1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3.2.1 compare the trip features before and after filter with different thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_nodes=data_merged_nodes[['trip_miles', 'trip_seconds', 'percent_time_chicago', \n",
    "                                'percent_distance_chicago']].describe().drop('count')\n",
    "filter_routes_diff500=data_merged_routes_diff500[['trip_miles', 'trip_seconds', 'percent_time_chicago',\n",
    "                            'percent_distance_chicago']].describe().drop('count')\n",
    "filter_routes_diff1000=data_merged_routes_diff1000[['trip_miles', 'trip_seconds', 'percent_time_chicago',\n",
    "                            'percent_distance_chicago']].describe().drop('count')\n",
    "filter_routes_diff1610=data_merged_routes_diff1610[['trip_miles', 'trip_seconds', 'percent_time_chicago',\n",
    "                            'percent_distance_chicago']].describe().drop('count')\n",
    "m1=filter_nodes.merge(filter_routes_diff500, left_index=True, right_index=True, suffixes=['_filter_nodes', '_filter_routes_diff500'])\n",
    "m2=filter_routes_diff1000.merge(filter_routes_diff1610, left_index=True, right_index=True, suffixes=['_filter_routes_diff1000', '_filter_routes_diff1610'])\n",
    "compare=m1.merge(m2, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trip_miles comparison\n",
    "com_trip_miles=compare[['trip_miles_filter_nodes',\n",
    "                        'trip_miles_filter_routes_diff500',\n",
    "                        'trip_miles_filter_routes_diff1000',\n",
    "                        'trip_miles_filter_routes_diff1610'\n",
    "        ]]\n",
    "com_trip_miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_trip_seconds=compare[['trip_seconds_filter_nodes', \n",
    "                          'trip_seconds_filter_routes_diff500',\n",
    "                          'trip_seconds_filter_routes_diff1000',\n",
    "                          'trip_seconds_filter_routes_diff1610'\n",
    " ]]\n",
    "com_trip_seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_percent_time_chicago=compare[['percent_time_chicago_filter_nodes',\n",
    "                                  'percent_time_chicago_filter_routes_diff500',\n",
    "                                  'percent_time_chicago_filter_routes_diff1000',\n",
    "                                  'percent_time_chicago_filter_routes_diff1610'\n",
    "     ]]\n",
    "com_percent_time_chicago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_percent_distance_chicago=compare[['percent_distance_chicago_filter_nodes',\n",
    "                                      'percent_distance_chicago_filter_routes_diff500',\n",
    "                                      'percent_distance_chicago_filter_routes_diff1000',\n",
    "                                      'percent_distance_chicago_filter_routes_diff1610'\n",
    "   ]]\n",
    "com_percent_distance_chicago"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis: \n",
    "- trip_miles statistical features have been changed obviously after the route filter with different thresholds. However, we can see that the 1610m threshod doesnt have a significant difference from the 500m threshod, which implies the retained trips by using the 500m threshold already kept most of the segment types. Thus, we can consider using the 500m threshod for the route filtering.\n",
    "- trip_seconds statistical features dont have significant changes after the route filter with different thresholds, which makes sense because a lot of factors can contribute to the travel time, like the weather, the traffic, and some unexpected events, so even if we have lost some diversity of road types after the route filter, the trip_seconds might still remain similar.\n",
    "- both percent_time_chicago and percent_distance_chicago are retained well after route filter with different thresholds, which makes sense if most of  the trips occurred within Chicago city limit. \n",
    "\n",
    "Conclusion:\n",
    "- it seems 500m threshold works well by comparing the trip related features before and after filter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3.2.2 compare the temporal traffic pattern before and after filter with the 500m threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the timestamp columns into datetime object\n",
    "df['trip_start_timestamp']=pd.to_datetime(df['trip_start_timestamp'])\n",
    "df['trip_end_timestamp']=pd.to_datetime(df['trip_end_timestamp'])\n",
    "data_merged_routes_diff500['trip_start_timestamp']=pd.to_datetime(data_merged_routes_diff500['trip_start_timestamp'])\n",
    "data_merged_routes_diff500['trip_end_timestamp']=pd.to_datetime(data_merged_routes_diff500['trip_end_timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract year, month, day of week, hour features.\n",
    "df['year']=df['trip_start_timestamp'].dt.year\n",
    "df['month']=df['trip_start_timestamp'].dt.month\n",
    "df['dayname']=df['trip_start_timestamp'].dt.day_name()\n",
    "df['hour_start']=df['trip_start_timestamp'].dt.hour\n",
    "df['hour_end']=df['trip_end_timestamp'].dt.hour\n",
    "data_merged_routes_diff500['year']=data_merged_routes_diff500['trip_start_timestamp'].dt.year\n",
    "data_merged_routes_diff500['month']=data_merged_routes_diff500['trip_start_timestamp'].dt.month\n",
    "data_merged_routes_diff500['dayname']=data_merged_routes_diff500['trip_start_timestamp'].dt.day_name()\n",
    "data_merged_routes_diff500['hour_start']=data_merged_routes_diff500['trip_start_timestamp'].dt.hour\n",
    "data_merged_routes_diff500['hour_end']=data_merged_routes_diff500['trip_end_timestamp'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the yearly temporal traffic pattern between the original dataset df without any filtering and the dataset after both nodes and routes filter\n",
    "org_year=df['year'].value_counts(normalize=True)\n",
    "routes500_year=data_merged_routes_diff500['year'].value_counts(normalize=True)\n",
    "pd.DataFrame({'org_year': org_year, 'routes500_year': routes500_year})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the monthly temporal traffic pattern \n",
    "org_month=df['month'].value_counts(normalize=True).sort_index()\n",
    "routes500_month=data_merged_routes_diff500['month'].value_counts(normalize=True)\n",
    "pd.DataFrame({'org_month': org_month,  \n",
    "              'routes500_month': routes500_month,\n",
    "              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the day of week temporal traffic pattern \n",
    "org_dayname=df['dayname'].value_counts(normalize=True).reindex(['Monday', 'Tuesday', 'Wednesday',\n",
    "                                                    'Thursday', 'Friday','Saturday', 'Sunday'])\n",
    "                                                                                               \n",
    "routes500_dayname=data_merged_routes_diff500['dayname'].value_counts(normalize=True).reindex(['Monday',\n",
    "                                                                                                'Tuesday', 'Wednesday','Thursday', 'Friday','Saturday', 'Sunday'])\n",
    "\n",
    "pd.DataFrame({'org_dayname': org_dayname, \n",
    "              'routes500_dayname': routes500_dayname})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the hourly temporal traffic pattern \n",
    "org_hour_start=df['hour_start'].value_counts(normalize=True).sort_index()\n",
    "routes500_hour_start=data_merged_routes_diff500['hour_start'].value_counts(normalize=True).sort_index()\n",
    "pd.DataFrame({'org_hour_start': org_hour_start, 'routes500_hour_start': routes500_hour_start,\n",
    "              })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "- it seems the temporal pattern has been retained well after the filter.\n",
    "- since the temporal features are well retained, the weather related features are also supposed to be well retained.\n",
    "- obvious difference in trip related features but high similarity in temporal patterns implies the remained trips after filter are common trips with good representative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data filted by both nodes and routes to a csv file for an easier access in the future\n",
    "data_merged_routes_diff500.to_csv('Chicago_routes_filter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged_routes_diff500=pd.read_csv('Chicago_routes_filter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#averge trip duration in min \n",
    "trip_duration_min=data_merged_routes_diff500['trip_seconds'].mean()/60\n",
    "trip_duration_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2 Create Segment Level Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Cluster all road segments of Chicago into regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to remove the list object \n",
    "def remove_list_obj (col):\n",
    "    col=col.apply(lambda x: tuple(x) if isinstance(x, list) else x)\n",
    "    return col\n",
    "# apply this function to edges gdf for easier operation in the following\n",
    "edges=edges.apply(remove_list_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract centroids of each road segment in edges\n",
    "edges['centroid'] = edges['geometry'].centroid\n",
    "edges['x'] = edges['centroid'].x\n",
    "edges['y'] = edges['centroid'].y\n",
    "\n",
    "#Use the centroids' coordinates to cluster all road segments of Chicago into regions.\n",
    "n_clusters = 29  \n",
    "# Apply K-Means\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "edges['region'] = kmeans.fit_predict(edges[['x', 'y']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: I used 29 because I referred to the region division information from website https://data.cityofchicago.org/api/assets/3F039704-BD76-4E6E-8E42-5F2BB01F0AF8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the edges in edges gdf and update G with region cluster  so that we can extract this information from G in the following\n",
    "edges=edges.reset_index()\n",
    "for _, row in edges.iterrows():\n",
    "    u, v, region = row['u'], row['v'], row['region']\n",
    "    for key in G[u][v]:\n",
    "        G[u][v][key]['region'] = region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add Geometry from edge GeoDataFrame to G for the plot in the following\n",
    "for idx, row in edges.iterrows():\n",
    "    u, v, key = row['u'], row['v'], row['key']\n",
    "    G[u][v][key]['geometry'] = row['geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot road segments on OSMnx, colored by their cluster (region)\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "# Calculate and plot region labels\n",
    "region_centroids = edges.groupby('region')['geometry'].apply(lambda x: x.unary_union.centroid)\n",
    "\n",
    "for region, centroid in region_centroids.items():\n",
    "    ax.text(centroid.x, centroid.y, f'region{region}',  # Adjust position slightly\n",
    "            fontsize=6, ha='center', va='center', color='black', \n",
    "            bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "\n",
    "edges.plot(ax=ax, column='region', cmap='tab20', legend=True, linewidth=0.8, legend_kwds={'label': \"Region Clusters\"})\n",
    "\n",
    "# Add title\n",
    "plt.title('Road Segments Clustered into 29 Regions (K-Means)', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Extract the road segments details from G "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe called data_merged_routes_diff500_chopped to only contain the unique routes\n",
    "data_merged_routes_diff500_chopped=data_merged_routes_diff500.drop_duplicates(['shortest_route'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to extract the road segments details of each shortest route from G\n",
    "def extract_road_segments_details(row):\n",
    "   # if the data was obtained by reading a csv file, the list of nodes would be string. Otherwise, it is a list of integers and can be used directly.   \n",
    "    if isinstance(row['shortest_route'], str):\n",
    "        shortest_route_str=row['shortest_route']\n",
    "        nodes_str=re.findall(r'\\d+', shortest_route_str)\n",
    "        nodes_int=[int(x) for x in nodes_str]\n",
    "    else:\n",
    "        nodes_int=row['shortest_route']\n",
    "    road_segments = []\n",
    "    for u, v in zip(nodes_int[:-1], nodes_int[1:]):\n",
    "        # by default, it wll be multi-edges between u and v \n",
    "        edgs = G[u][v]\n",
    "        # If there's only one edge, take it\n",
    "        if len(edgs) == 1:\n",
    "            edge_data = list(edgs.values())[0]\n",
    "        else:\n",
    "            # For multiple edges, select the one with the minimum 'length' since this matches how we calculated the shortest route earlier\n",
    "            edge_data = min(edgs.values(), key=lambda x: x.get('length', np.inf))\n",
    "        # feed (u,v) info into edge_data which didnt contain it by default\n",
    "        edge_data['u'] = u\n",
    "        edge_data['v'] = v\n",
    "        road_segments.append((u, v, edge_data))\n",
    "        \n",
    "    edge_data_ls=[]\n",
    "    free_travel_time_sec_ls=[]\n",
    "    for segment in road_segments:\n",
    "        u,v,edge_data=segment\n",
    "        edge_data_ls.append(edge_data)\n",
    "        free_travel_time_sec_ls.append(edge_data.get('travel_time', np.nan))\n",
    "    free_travel_time_total_sec=sum(free_travel_time_sec_ls)\n",
    "    return (free_travel_time_total_sec,  edge_data_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# application\n",
    "data_merged_routes_diff500_chopped[['free_travel_time_total_sec', \n",
    "                                                               'edge_data_ls', \n",
    "                                                              ]]=data_merged_routes_diff500_chopped.apply(lambda row:\n",
    "                                                                                                                          pd.Series(extract_road_segments_details(row)),\n",
    "                                                                                                                          axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the 'free_travel_time_total_sec' and 'edge_data_ls' columns to data_merged_routes_diff500 and rename it as trips_segs_merge\n",
    "data_merged_routes_diff500_chopped_subcols=data_merged_routes_diff500_chopped[['free_travel_time_total_sec', \n",
    "                                                               'edge_data_ls', 'shortest_route']]\n",
    "trips_segs_merge=pd.merge(data_merged_routes_diff500, data_merged_routes_diff500_chopped_subcols, on='shortest_route')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Display the road segments in our dataset on OSMnx graph and check their coverage ratio in each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all road segments extracted from the existing dataset into a list \n",
    "shortest_routes_segs_ls=[]\n",
    "for x in data_merged_routes_diff500_chopped['edge_data_ls'].values:\n",
    "    shortest_routes_segs_ls.extend(x)\n",
    "\n",
    "# use chuncks to deal with memory error\n",
    "chunk_size = 100000  # Adjust based on available memory\n",
    "chunks = []\n",
    "for i in range(0, len(shortest_routes_segs_ls), chunk_size):\n",
    "    chunk = pd.DataFrame(shortest_routes_segs_ls[i:i + chunk_size])\n",
    "    chunks.append(chunk)\n",
    "\n",
    "shortest_routes_segs_df = pd.concat(chunks, ignore_index=True)\n",
    "# remove list object \n",
    "shortest_routes_segs_df=shortest_routes_segs_df.apply(remove_list_obj)\n",
    "# remove duplicated segments\n",
    "shortest_routes_segs_df_group=shortest_routes_segs_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the road segments in our dataset on OSMnx graph to check their coverage\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "# define a function to plot the segments from the calculated shortest routes in our dataset\n",
    "def plot_shortest_routes(row, graph=G, ax=ax, color='black', linewidth=0.5):\n",
    "    geometry=row['geometry']\n",
    "    ax.plot(*geometry.xy, color=color, linewidth=linewidth)\n",
    "# plot all the edges in Chicago with their region label\n",
    "\n",
    "region_centroids = edges.groupby('region')['geometry'].apply(lambda x: x.unary_union.centroid)\n",
    "for region, centroid in region_centroids.items():\n",
    "    ax.text(centroid.x, centroid.y, f'region{region}',  # Adjust position slightly\n",
    "            fontsize=6, ha='center', va='center', color='black', \n",
    "            bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "\n",
    "edges.plot(ax=ax, column='region', cmap='tab20', legend=True, linewidth=0.8, legend_kwds={'label': \"Region Clusters\"})\n",
    "# Add title\n",
    "plt.title('Road Segments Coverage Check', fontsize=16)\n",
    "#apply the defined plot function to the extracted segments df\n",
    "shortest_routes_segs_df_group.apply(lambda row: plot_shortest_routes(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Complete the segment level traffic information   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to create segment level dataframe while distributing the trip_secs to each road segment proportionally\n",
    "def create_seg_level_df (row):\n",
    "    total_free_travel_time=row['free_travel_time_total_sec']\n",
    "    X=pd.DataFrame(row['edge_data_ls'])[[ 'lanes',  'highway', \n",
    "        'speed_kph', 'travel_time', 'region',  'u', 'v', 'junction',\n",
    "       'bridge', 'tunnel', 'access']]\n",
    "    X['inferred_travel_time_sec']=np.maximum(X['travel_time'], X['travel_time']/total_free_travel_time*row['trip_seconds'])\n",
    "    X[['Unnamed: 0', 'trip_start_timestamp', 'trip_end_timestamp', \n",
    "       'temp', 'dwpt', 'rhum', 'prcp', 'wdir', 'wpgt', 'pres']]=row[['Unnamed: 0', 'trip_start_timestamp', 'trip_end_timestamp', \n",
    "       'temp', 'dwpt', 'rhum', 'prcp', 'wdir', 'wpgt', 'pres']]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: if you run the following cell in notebook, it probably will cause page snapped (the data size is over 6GB). You can run it in Anaconda prompt instead**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 100  # Adjust based on available memory\n",
    "# Open the file in append mode and write chunks incrementally\n",
    "for i in range(0, len(trips_segs_merge), chunk_size):\n",
    "    chunk = trips_segs_merge.iloc[i:i + chunk_size]\n",
    "    chunk_result = pd.concat(chunk.apply(create_seg_level_df, axis=1).to_list(), ignore_index=True)   \n",
    "    # Write chunk results to file\n",
    "    chunk_result.to_csv('segment_level_data.csv', mode='a', index=False, header=(i == 0));  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3 Check the Temporal Traffic Pattern Completeness for Each Region "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV lazily with Dask due to the large size of the dataset\n",
    "ddf = dd.read_csv('segment_level_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the huge dataset into 29 smaller datasets representing 29 regions\n",
    "for region in range(29):\n",
    "    print(region)\n",
    "    filtered_region = ddf[ddf['region'] == region]   \n",
    "    # Save the filtered data directly to a file\n",
    "    filtered_region.to_csv(f'filtered_region_{region}.csv', single_file=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trip level dataset for comparison\n",
    "data_merged_routes_diff500=pd.read_csv('Chicago_routes_filter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the timestamp columns into datetime object and extract year, month, day of week, and hour temporal infornation\n",
    "data_merged_routes_diff500['trip_start_timestamp']=pd.to_datetime(data_merged_routes_diff500['trip_start_timestamp'])\n",
    "data_merged_routes_diff500['trip_end_timestamp']=pd.to_datetime(data_merged_routes_diff500['trip_end_timestamp'])\n",
    "data_merged_routes_diff500['year']=data_merged_routes_diff500['trip_start_timestamp'].dt.year\n",
    "data_merged_routes_diff500['month']=data_merged_routes_diff500['trip_start_timestamp'].dt.month\n",
    "data_merged_routes_diff500['dayname']=data_merged_routes_diff500['trip_start_timestamp'].dt.day_name()\n",
    "data_merged_routes_diff500['hour_start']=data_merged_routes_diff500['trip_start_timestamp'].dt.hour\n",
    "data_merged_routes_diff500['hour_end']=data_merged_routes_diff500['trip_end_timestamp'].dt.hour\n",
    "# generate yearly traffic pattern \n",
    "total_yearly=data_merged_routes_diff500['year'].value_counts(normalize=True).sort_index()\n",
    "yearly_comp=pd.DataFrame({'total_yearly': total_yearly})\n",
    "# generate monthly traffic pattern \n",
    "total_monthly=data_merged_routes_diff500['month'].value_counts(normalize=True).sort_index()\n",
    "monthly_comp=pd.DataFrame({'total_monthly': total_monthly})\n",
    "# generate day of week traffic pattern \n",
    "total_daily=data_merged_routes_diff500['dayname'].value_counts(normalize=True).reindex(['Monday', 'Tuesday','Wednesday', 'Thursday',                                                                                         'Friday', 'Saturday', 'Sunday'], axis=1)\n",
    "daily_comp=pd.DataFrame({'total_daily': total_daily})\n",
    "# generate hourly traffic pattern \n",
    "total_hourly=data_merged_routes_diff500['hour_start'].value_counts(normalize=True).sort_index()\n",
    "hourly_comp=pd.DataFrame({'total_hourly': total_hourly})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each region for the same operation as above\n",
    "for region in range(29):\n",
    "    filtered_region=pd.read_csv(f'filtered_region_{region}.csv')   \n",
    "    filtered_region['trip_start_timestamp']=pd.to_datetime(filtered_region['trip_start_timestamp'])\n",
    "    filtered_region['trip_end_timestamp']=pd.to_datetime(filtered_region['trip_end_timestamp'])\n",
    "    filtered_region['year']=filtered_region['trip_start_timestamp'].dt.year\n",
    "    filtered_region['month']=filtered_region['trip_start_timestamp'].dt.month\n",
    "    filtered_region['dayname']=filtered_region['trip_start_timestamp'].dt.day_name()\n",
    "    filtered_region['hour_start']=filtered_region['trip_start_timestamp'].dt.hour\n",
    "    filtered_region['hour_end']=filtered_region['trip_end_timestamp'].dt.hour\n",
    "    \n",
    "    # temporal pattern comparison\n",
    "    region_yearly=filtered_region['year'].value_counts(normalize=True).sort_index()\n",
    "    yearly_comp[f'region_{region}_yearly']=region_yearly\n",
    "    region_monthly=filtered_region['month'].value_counts(normalize=True).sort_index()\n",
    "    monthly_comp[f'region_{region}_monthly']=region_monthly\n",
    "    region_daily=filtered_region['dayname'].value_counts(normalize=True).reindex(['Monday', 'Tuesday','Wednesday', 'Thursday', \n",
    "                                                                                  'Friday', 'Saturday', 'Sunday'], axis=1)\n",
    "    daily_comp[f'region_{region}_daily']=region_daily\n",
    "    region_hourly=filtered_region['hour_start'].value_counts(normalize=True).sort_index()\n",
    "    hourly_comp[f'region_{region}_hourly']=region_hourly \n",
    "    # Clear memory by deleting the current region's dataframe\n",
    "    del filtered_region\n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    print(f\"Processed and cleared region {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_comp.plot(kind='line')\n",
    "plt.ylabel('normalized traffic amount')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))  # Position legend outside the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_comp.plot(kind='line')\n",
    "plt.ylabel('normalized traffic amount')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_comp.loc[:, 'region_0_hourly':].plot(kind='line')\n",
    "hourly_comp.loc[:, 'total_hourly'].plot(kind='line', color='black')\n",
    "plt.ylabel('normalized traffic amount')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "- we can see each region follows a similar temporal traffic patten to the overall dataset by monthly, daily, and hourly, which means all regions retain complete temporal traffic information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4 Road Segments Clustering within Each Region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Data preparation for KMeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timeframe here represents morning, afternoon, and night timeframes\n",
    "regions_timeframe={}\n",
    "regions_hourly={}\n",
    "# Loop through each region\n",
    "for region in range(29):\n",
    "    filtered_region=pd.read_csv(f'filtered_region_{region}.csv')   \n",
    "    filtered_region['trip_start_timestamp']=pd.to_datetime(filtered_region['trip_start_timestamp'])\n",
    "    filtered_region['trip_end_timestamp']=pd.to_datetime(filtered_region['trip_end_timestamp'])\n",
    "    filtered_region['year']=filtered_region['trip_start_timestamp'].dt.year\n",
    "    filtered_region['month']=filtered_region['trip_start_timestamp'].dt.month\n",
    "    filtered_region['dayname']=filtered_region['trip_start_timestamp'].dt.day_name()\n",
    "    filtered_region['hour_start']=filtered_region['trip_start_timestamp'].dt.hour\n",
    "    filtered_region['hour_end']=filtered_region['trip_end_timestamp'].dt.hour\n",
    "    filtered_region['inferred_travel_time_sec'] = np.maximum(filtered_region['inferred_travel_time_sec'], filtered_region['travel_time'])\n",
    "# add a new column 'morning_afternoon_night' to group hours of day to morning, afternoon, and night time frames \n",
    "    filtered_region['morning_afternoon_night'] = filtered_region['hour_start'].apply(\n",
    "    lambda x: 'morning' if 6 <= x < 12 \n",
    "              else 'afternoon' if 12 <= x <18 \n",
    "              else 'night' )\n",
    "   # aggregate the inferred_travel_time_sec by each timeframe for each segment \n",
    "    filtered_region_timeframe=filtered_region.groupby(\n",
    "    ['u', 'v', 'morning_afternoon_night']\n",
    "    )['inferred_travel_time_sec'].median().unstack().reindex(['morning', 'afternoon', 'night'], axis=1)\n",
    "    regions_timeframe[f'filtered_region_{region}_timeframe']=filtered_region_timeframe\n",
    "    # aggregate the inferred_travel_time_sec by hourly for each segment \n",
    "    filtered_region_hourly=filtered_region.groupby(\n",
    "    ['u', 'v', 'hour_start']\n",
    "    )['inferred_travel_time_sec'].median().unstack()\n",
    "    regions_hourly[f'filtered_region_{region}_hourly']=filtered_region_hourly\n",
    "    \n",
    "    # Clear memory by deleting the current region's dataframe\n",
    "    del filtered_region\n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    print(f\"Processed and cleared region {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to fill NaN values in the hourly traffic distribution for each segment:\n",
    "## to fill each hourly missing data by the corresponding morning or afternoon or night aggregated traffic data for each segment \n",
    "## if the morning or afternoon or night aggregated traffic data is also missing, fill it by using the average over the existing other timeframes.\n",
    "morning_hours=[6,7,8,9,10,11]\n",
    "afternoon_hours=[12,13,14,15,16,17]\n",
    "night_hours=[18,19,20,21,22,23,0,1,2,3,4,5]\n",
    "def fill_na(row):\n",
    "    for hour_col in range(24):\n",
    "        if pd.isna(row[hour_col]):\n",
    "            if hour_col in morning_hours:\n",
    "                if not pd.isna(row['morning']):\n",
    "                    row[hour_col]=row['morning']\n",
    "                else:\n",
    "                    row[hour_col]=np.nanmean([row['afternoon'], row['night']])\n",
    "            elif hour_col in afternoon_hours:\n",
    "                if not pd.isna(row['afternoon']):\n",
    "                    row[hour_col]=row['afternoon']\n",
    "                else:\n",
    "                    row[hour_col]=np.nanmean([row['morning'], row['night']])\n",
    "            else:\n",
    "                if not pd.isna(row['night']):\n",
    "                    row[hour_col]=row['night']\n",
    "                else:\n",
    "                    row[hour_col]=np.nanmean([row['morning'], row['afternoon']])\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#application\n",
    "regions_hourly_fillna={}\n",
    "for region in range(29):\n",
    "    filtered_region_merge=pd.merge(regions_hourly[f'filtered_region_{region}_hourly'],\n",
    "                                              regions_timeframe[f'filtered_region_{region}_timeframe'], left_index=True, right_index=True)\n",
    "    regions_hourly_fillna[f'filtered_region_{region}_hourly_fillna']=filtered_region_merge.apply(fill_na, axis=1).loc[:, :23]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Use elbow method to determine K for each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region=0\n",
    "X=regions_hourly_fillna[f'filtered_region_{region}_hourly_fillna']\n",
    "# Extract hourly travel time data\n",
    "hourly_data = X.values\n",
    "\n",
    "# Normalize each segment by its total travel time to capture patterns\n",
    "pattern_data = hourly_data / np.sum(hourly_data, axis=1, keepdims=True)\n",
    "\n",
    "# Add the sum (scale) back as a feature\n",
    "scale_feature = np.sum(hourly_data, axis=1).reshape(-1, 1)\n",
    "\n",
    "# Combine pattern and scale features\n",
    "combined_features = np.hstack((pattern_data, scale_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the elbow plot\n",
    "model = KMeans()\n",
    "plot_elbow_curve(model, combined_features, cluster_ranges=range(1, min(20, len(X))), figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "- from the above plot, we can start with the K that corresponds to the last elbow. In the above example where region=0, K=6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use KMeans to do clustering\n",
    "kmeans = KMeans(n_clusters=6, random_state=42) \n",
    "X['cluster']=kmeans.fit_predict(combined_features) \n",
    "X['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory by deleting the old region's dataframe before creating the new region in the following cell\n",
    "del filtered_region\n",
    "del merge\n",
    "# Force garbage collection\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a new region data\n",
    "filtered_region=pd.read_csv(f'filtered_region_{region}.csv')   \n",
    "filtered_region['trip_start_timestamp']=pd.to_datetime(filtered_region['trip_start_timestamp'])\n",
    "filtered_region['trip_end_timestamp']=pd.to_datetime(filtered_region['trip_end_timestamp'])\n",
    "filtered_region['year']=filtered_region['trip_start_timestamp'].dt.year\n",
    "filtered_region['month']=filtered_region['trip_start_timestamp'].dt.month\n",
    "filtered_region['dayname']=filtered_region['trip_start_timestamp'].dt.day_name()\n",
    "filtered_region['hour_start']=filtered_region['trip_start_timestamp'].dt.hour\n",
    "filtered_region['hour_end']=filtered_region['trip_end_timestamp'].dt.hour\n",
    "filtered_region['inferred_travel_time_sec'] = np.maximum(filtered_region['inferred_travel_time_sec'], filtered_region['travel_time'])\n",
    "filtered_region=filtered_region.set_index(['u', 'v'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the cluster column to the loaded region data\n",
    "merge=pd.merge(filtered_region, X[['cluster']], left_index=True, right_index=True)\n",
    "merge['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the hourly aggregated inferred_travel_time_sec for each cluster \n",
    "time='hour_start'\n",
    "df_plt=merge.groupby(['cluster', time])['inferred_travel_time_sec'].median().unstack().T\n",
    "df_plt['total']=merge.groupby(time)['inferred_travel_time_sec'].median()\n",
    "df_plt.iloc[:,:-1].plot(marker='o')\n",
    "df_plt['total'].plot(marker='*', color='black', label='total')\n",
    "plt.ylabel('aggregated_inferred_travel_time_sec')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the hourly traffic pattern for each cluster\n",
    "df_plt2=merge.groupby('cluster')[time].value_counts(normalize=True).unstack().T\n",
    "df_plt2['total']=merge[time].value_counts(normalize=True)\n",
    "df_plt2.iloc[:,:-1].plot(marker='*')\n",
    "df_plt2['total'].plot(marker='+', color='black', label='total')\n",
    "plt.ylabel('traffic_amount_ratio')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "- combine with the above two plots, we can determine whether to adjust K further. For example, if we noticed there exist clusters close to each other in the first plot (the inferred_travel_time_sec hourly distribution), we may want to reduce K, and if we noticed there exist incomplete lines for some clusters in both plots, we can also consider reducing K to try to merge these outliers with other clusters if reasonably. Our objective is to see  well separated complete lines in the first plot and high overlapped complete lines in the second plot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster Result: based on checking the clustering result for each of 29 regions one by one, I got the following result. There exist some outliers I didnt merge into other clusters (marked by red) since I didnt want to sacrifice the clustering accuracy because of them. I will figure out a way for them in the following model part.\n",
    "\n",
    "---\n",
    "\n",
    "###### Region 0: 6 Clusters\n",
    "- 0:    96,449 rows\n",
    "- 4:   54,403\n",
    "- 3:    50,219\n",
    "- 2:    37,644\n",
    "- 1:     3,517\n",
    "- 5:     1,402\n",
    "\n",
    "---\n",
    "\n",
    "###### Region 1: 5 Clusters\n",
    "\n",
    "- 2:    332,363\n",
    "- 0:    161,706\n",
    "- 4:     80,164\n",
    "- 1:     76,591\n",
    "- 3:      7,188\n",
    "---\n",
    "\n",
    "###### Region 2: 4 Clusters\n",
    "- 1:    3,380,709\n",
    "- 3:    2,380,233\n",
    "- 0:    1,303,532\n",
    "- 2:      88,891\n",
    "---\n",
    "\n",
    "###### Region 3: 5 Clusters\n",
    "- 1:    401,577\n",
    "- 3:    209,233\n",
    "- 2:    154,352\n",
    "- 0:     89,029\n",
    "- 4:      7,300\n",
    "---\n",
    "\n",
    "###### Region 4: 5 Clusters\n",
    "- 0    700,478\n",
    "- 2    532,191\n",
    "- 1    281,492\n",
    "- 4     43,570\n",
    "- 3      5,098\n",
    "---\n",
    "\n",
    "###### Region 5: 5 Clusters\n",
    "- 0    37,532\n",
    "- 4     7,376\n",
    "- 2     7,071\n",
    "- 3     6,707\n",
    "- 1     5,446\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "###### Region 6: 5 Clusters\n",
    "\n",
    "- 4    440,870\n",
    "- 2    270,005\n",
    "- 0    146,883\n",
    "- 1    117,562\n",
    "- 3     10,666\n",
    "\n",
    "---\n",
    "\n",
    "###### Region 7: 5 Clusters\n",
    "\n",
    "- 4    36,195\n",
    "- 0    34,550\n",
    "- 3    18,996\n",
    "- 2    12,045\n",
    "- 1     3,185\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "###### Region 8: 4 Clusters\n",
    "- 2    372,019\n",
    "- 0    221,825\n",
    "- 1    100,684\n",
    "- 3     36,581\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "###### Region 9: 5 Clusters\n",
    "- 1    233,719\n",
    "- 0     62,952\n",
    "- 2     42,501\n",
    "- 4     26,054\n",
    "- 3      4,746\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "###### Region 10: 5 Clusters\n",
    "- 1    44,793\n",
    "- 0    26,938\n",
    "- 3    11,885\n",
    "- 2     2,402\n",
    "- 4     **<span style=\"color:red;\">13</span>**\n",
    "---\n",
    "\n",
    "###### Region 11: 5 Clusters\n",
    "- 2    40,320\n",
    "- 0    11,656\n",
    "- 3     1,609\n",
    "- 1       **<span style=\"color:red;\">125</span>**  \n",
    "- 4       **<span style=\"color:red;\">71</span>**  \n",
    "---\n",
    "\n",
    "###### Region 12: 5 Clusters\n",
    "\n",
    "- 0    116,669\n",
    "- 4     48,947\n",
    "- 2     37,257\n",
    "- 1       518\n",
    "- 3     **<span style=\"color:red;\">3</span>**\n",
    "\n",
    "---\n",
    "\n",
    "###### Region 13: 5 Clusters\n",
    "\n",
    "- 2    6,148,668\n",
    "- 1    3,088,207\n",
    "- 0    3,056,195\n",
    "- 3     619,525\n",
    "- 4      98,524\n",
    "\n",
    "---\n",
    "\n",
    "###### Region 14: 6 Clusters\n",
    "\n",
    "- 5    1,157,573\n",
    "- 0     765,385\n",
    "- 4     525,156\n",
    "- 3     520,131\n",
    "- 1     287,293\n",
    "- 2      11,593\n",
    "\n",
    "---\n",
    "\n",
    "###### Region 15: 5 Clusters\n",
    "- 4    137,890\n",
    "- 1     99,674\n",
    "- 0     40,213\n",
    "- 3      7,868\n",
    "- 2      **<span style=\"color:red;\">8</span>**\n",
    "\n",
    "---\n",
    "\n",
    "###### Region 16: 5 Clusters\n",
    "- 0    35,594\n",
    "- 3    19,156\n",
    "- 4    10,396\n",
    "- 1     4,800\n",
    "- 2     2,892\n",
    "\n",
    "---\n",
    "\n",
    "###### Region 17: 5 Clusters\n",
    "\n",
    "- 3    451983\n",
    "- 1    379076\n",
    "- 0    351754\n",
    "- 2    183072\n",
    "- 4     20732\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "###### Region 18: 6 Clusters\n",
    "\n",
    "- 5    128354\n",
    "- 0    100929\n",
    "- 2     85153\n",
    "- 1     78779\n",
    "- 4     35143\n",
    "- 3      1168\n",
    "\n",
    "---\n",
    "\n",
    "###### Region 19: 4 Clusters\n",
    "\n",
    "- 2    139,630\n",
    "- 0     94,054\n",
    "- 3     28,192\n",
    "- 1      3,220\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "###### Region 20: 5 Clusters\n",
    "- 4    180024\n",
    "- 0     70587\n",
    "- 1     43093\n",
    "- 2      2896\n",
    "- 3      **<span style=\"color:red;\">1</span>**\n",
    "\n",
    "---\n",
    "\n",
    "###### Region 21: 5 Clusters\n",
    "- 0    1,289,560\n",
    "- 4    1,005,728\n",
    "- 2     450,651\n",
    "- 1     237,195\n",
    "- 3      38,688\n",
    "\n",
    "---\n",
    "\n",
    "###### Region 22: 5 Clusters \n",
    "- 2    218,782\n",
    "- 0    105,606\n",
    "- 1     80,935\n",
    "- 4     17,964\n",
    "- 3      2,342\n",
    "---\n",
    "\n",
    "###### Region 23: 4 Clusters\n",
    "- 3    12658\n",
    "- 1     8365\n",
    "- 0     5909\n",
    "- 2    **<span style=\"color:red;\">425</span>**  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "###### Region 24: 5 Clusters\n",
    "- 0    304,168\n",
    "- 4    170,309\n",
    "- 3     59,218\n",
    "- 2      4,870\n",
    "- 1      2,257\n",
    "\n",
    "---\n",
    "\n",
    "###### Region 25: 5 Clusters\n",
    "- 1    1,045,304\n",
    "- 3     546,643\n",
    "- 2     412,322\n",
    "- 0     166,239\n",
    "- 4      11,376\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "###### Region 26: 6 Clusters \n",
    "- 1    542,353\n",
    "- 5    176,423\n",
    "- 0    108,009\n",
    "- 4     78,770\n",
    "- 3     16,192\n",
    "- 2      **<span style=\"color:red;\">6</span>**\n",
    "\n",
    "---\n",
    "\n",
    "###### Region 27: 5 Clusters\n",
    "- 1    398,144\n",
    "- 3    200,025\n",
    "- 0    154,587\n",
    "- 2     70,160\n",
    "- 4       961\n",
    "\n",
    "---\n",
    "\n",
    "###### Region 28: 4 Clusters\n",
    "- 2    570\n",
    "- 0    223\n",
    "- 1     52\n",
    "- 3     20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Save the obtained cluster information for each region to the segment level dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result=[6,5,4,5,5,5,5,5,4,5,5,5,5,5,6,5,5,5,6,4,5,5,5,4,5,5,6,5,4]\n",
    "time='hour_start'\n",
    "regions_traveltime={}\n",
    "regions_traffic={}\n",
    "for region in range(29):\n",
    "    X=regions_hourly_fillna[f'filtered_region_{region}_hourly_fillna']\n",
    "    # Extract hourly travel time data\n",
    "    hourly_data=X.values\n",
    "    # Normalize each road segment by its total travel time to capture patterns\n",
    "    pattern_data=hourly_data/np.sum(hourly_data, axis=1, keepdims=True)\n",
    "    # Add the sum (scale) back as a feature\n",
    "    scale_feature=np.sum(hourly_data, axis=1).reshape(-1,1)\n",
    "    # Combine pattern and scale features\n",
    "    combined_features=np.hstack((pattern_data, scale_feature))\n",
    "    kmeans=KMeans(n_clusters=cluster_result[region], random_state=42)\n",
    "    X['cluster']=kmeans.fit_predict(combined_features)\n",
    "    filtered_region=pd.read_csv(f'filtered_region_{region}.csv')\n",
    "    filtered_region['trip_start_timestamp']=pd.to_datetime(filtered_region['trip_start_timestamp'])\n",
    "    filtered_region['trip_end_timestamp']=pd.to_datetime(filtered_region['trip_end_timestamp'])\n",
    "    filtered_region['year']=filtered_region['trip_start_timestamp'].dt.year\n",
    "    filtered_region['month']=filtered_region['trip_start_timestamp'].dt.month\n",
    "    filtered_region['dayname']=filtered_region['trip_start_timestamp'].dt.day_name()\n",
    "    filtered_region['hour_start']=filtered_region['trip_start_timestamp'].dt.hour\n",
    "    filtered_region['hour_end']=filtered_region['trip_end_timestamp'].dt.hour\n",
    "    filtered_region['inferred_travel_time_sec'] = np.maximum(filtered_region['inferred_travel_time_sec'], filtered_region['travel_time'])\n",
    "    filtered_region=filtered_region.set_index(['u', 'v'])\n",
    "    merge=pd.merge(filtered_region, X[['cluster']], left_index=True, right_index=True)\n",
    "    df_traveltime=merge.groupby(['cluster', time])['inferred_travel_time_sec'].median().unstack().T\n",
    "    df_traveltime['total']=merge.groupby(time)['inferred_travel_time_sec'].median()\n",
    "    regions_traveltime[f'region_{region}']=df_traveltime\n",
    "    df_traffic=merge.groupby('cluster')[time].value_counts(normalize=True).unstack().T\n",
    "    df_traffic['total']=merge[time].value_counts(normalize=True)\n",
    "    regions_traffic[f'region_{region}']=df_traffic\n",
    "    merge.to_csv(f'region_{region}_withcluster.csv', index=True)\n",
    "    # Clear memory by deleting the current region's dataframe\n",
    "    del filtered_region\n",
    "    del merge\n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    print(f\"Processed and cleared region {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the inferred travel time hourly distribution for each region with the total dataset\n",
    "fig, axs=plt.subplots(10, 3, figsize=(20,60))\n",
    "for region in range(29):\n",
    "    row_ind=region//3\n",
    "    col_ind=region%3\n",
    "    ax=axs[row_ind, col_ind]\n",
    "    regions_traveltime[f'region_{region}'].iloc[:,:-1].plot(marker='o', ax=ax)\n",
    "    regions_traveltime[f'region_{region}']['total'].plot(marker='*', color='black', label='total', ax=ax)\n",
    "    ax.set_title(f'Region {region}', fontsize=10)\n",
    "    ax.set_ylabel('aggregated_inferred_travel_time_sec', fontsize=8)\n",
    "    ax.legend(fontsize=8)\n",
    "# Hide unused subplot\n",
    "axs[9, 2].set_visible(False)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the hourly traffic pattern  for each region with the total dataset\n",
    "fig, axs=plt.subplots(10, 3, figsize=(20,60))\n",
    "for region in range(29):\n",
    "    row_ind=region//3\n",
    "    col_ind=region%3\n",
    "    ax=axs[row_ind, col_ind]\n",
    "    regions_traffic[f'region_{region}'].iloc[:,:-1].plot(marker='*', ax=ax)\n",
    "    regions_traffic[f'region_{region}']['total'].plot(marker='+', color='black', label='total', ax=ax)\n",
    "    ax.set_title(f'Region {region}', fontsize=10)\n",
    "    ax.set_ylabel('normalized traffic amount', fontsize=8)\n",
    "    ax.legend(fontsize=8)\n",
    "# Hide unused subplot\n",
    "axs[9, 2].set_visible(False)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Display the above plotted results onto map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1 Region data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a region dataset to display\n",
    "region=1\n",
    "region_withcluster=pd.read_csv(f'region_{region}_withcluster.csv')\n",
    "region_withcluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_data=region_withcluster.groupby(['cluster', 'u', 'v', 'hour_start'])[['inferred_travel_time_sec', 'travel_time']].median()\n",
    "region_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a congestion level column by comparing inferred_travel_time_sec with travel_time\n",
    "def calculate_congestion_level(row):\n",
    "    if pd.isna(row['inferred_travel_time_sec']):\n",
    "        ratio==1\n",
    "        return (ratio, 'lightgreen', 'free-flow')  # Mark NaN values as free flow\n",
    "    ratio = row['inferred_travel_time_sec'] / row['travel_time']\n",
    "    if ratio <= 1.2:\n",
    "        return (ratio, 'lightgreen', 'free_flow')  # Free flow\n",
    "    elif ratio <= 1.5:\n",
    "        return (ratio, 'green', 'slight congestion')  # Slight congestion\n",
    "    elif ratio <= 2.0:\n",
    "        return (ratio, '#E65100', 'medium congestion')  # Medium congestion\n",
    "    elif ratio >2:\n",
    "        return (ratio, 'darkred', 'severe congestion')  # Severe congestion\n",
    "#application\n",
    "region_data[['congestion_ratio', 'congestion_level', 'congestion_level_explanation']] = region_data.apply(lambda row: pd.Series(\n",
    "                                                                                                                                        calculate_congestion_level(row)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine 'highway' column to region_data to reflect the road type information on map\n",
    "m1=region_data.reset_index()\n",
    "m2=region_withcluster.drop_duplicates(['u','v'])[['u', 'v', 'highway']]\n",
    "merge=pd.merge(m1, m2, on=['u', 'v'])\n",
    "region_data=merge.set_index(['cluster', 'u', 'v','hour_start'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2 Plot road segments on map, colored by congestion level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an OSMnx graph for Chicago city\n",
    "place = 'Chicago, Illinois, USA'\n",
    "G = ox.graph_from_place(place, network_type='drive')\n",
    "\n",
    "#add speed limit and free flow travel_time attributes to the edges.\n",
    "G = ox.add_edge_speeds(G)\n",
    "G = ox.add_edge_travel_times(G)\n",
    "# create geo-dataframe for both nodes and edges \n",
    "nodes, edges = ox.graph_to_gdfs(G, nodes=True, edges=True)\n",
    "# add geometry from edges to G for plot in the following\n",
    "for idx, row in edges.reset_index().iterrows():\n",
    "    u, v, key = row['u'], row['v'], row['key']\n",
    "    G[u][v][key]['geometry'] = row['geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot road segments of a region with cluster and hour-based filtering on map\n",
    "def plot_region_with_filters(region_data, region_id, G):\n",
    "    # Initialize map centered on Chicago\n",
    "    m = folium.Map(location=[41.8781, -87.6298], zoom_start=11)\n",
    "\n",
    "    # Add a base layer with no visible data\n",
    "    folium.TileLayer(\"cartodbdark_matter\", name=\"No Data Visible\", control=True).add_to(m)\n",
    "\n",
    "    # Iterate through clusters and hours\n",
    "    clusters = region_data.index.get_level_values(\"cluster\").unique()\n",
    "\n",
    "    for cluster in clusters:\n",
    "        # Filter data for the current cluster \n",
    "        cluster_data = region_data[region_data.index.get_level_values(\"cluster\") == cluster]\n",
    "        for hour in range(24):  # Loop through hours from 0 to 23\n",
    "            hour_data = cluster_data[cluster_data.index.get_level_values(\"hour_start\") == hour]    \n",
    "            # Prepare GeoJSON data\n",
    "            geojson_data = {\"type\": \"FeatureCollection\", \"features\": []}\n",
    "\n",
    "            for index, row in hour_data.iterrows():\n",
    "                _, u, v,  hour_start = index\n",
    "                    \n",
    "                # Try to get the edge geometry\n",
    "                edgs = G[u][v]\n",
    "                if len(edgs) == 1:\n",
    "                    edge_data = list(edgs.values())[0]\n",
    "                else:\n",
    "                    edge_data = min(edgs.values(), key=lambda x: x.get(\"length\", float(\"inf\")))\n",
    "\n",
    "                # Extract coordinates from LineString\n",
    "                line_coords = list(edge_data[\"geometry\"].coords)\n",
    "\n",
    "                # Add road segment as a GeoJSON feature\n",
    "                feature = {\n",
    "                        \"type\": \"Feature\",\n",
    "                        \"geometry\": {\n",
    "                            \"type\": \"LineString\",\n",
    "                            \"coordinates\": line_coords\n",
    "                        },\n",
    "                        \"properties\": {\n",
    "                            \"congestion_level\": row[\"congestion_level\"],\n",
    "                            \"popup\": (\n",
    "                                f\"Inferred Travel Time: {row['inferred_travel_time_sec']} sec<br>\"\n",
    "                                f\"Free Flow Time: {row['travel_time']} sec<br>\"\n",
    "                                f\"Congestion: {row['congestion_level_explanation']}<br>\"\n",
    "                                f\"Congestion_ratio: {row['congestion_ratio']}<br>\"\n",
    "                                f\"roadtype: {row['highway']}\"\n",
    "                            )\n",
    "                        }\n",
    "                    }\n",
    "                geojson_data[\"features\"].append(feature)\n",
    "\n",
    "          \n",
    "                # Add GeoJson layer for the specific cluster and hour, hidden by default\n",
    "            folium.GeoJson(\n",
    "                    geojson_data,\n",
    "                    name=f\"Cluster {cluster}  | Hour {hour}\",  # Layer name for toggling\n",
    "                    style_function=lambda feature: {\n",
    "                            \"color\": feature[\"properties\"][\"congestion_level\"],\n",
    "                            \"weight\": 5,\n",
    "                            \"opacity\": 1.0\n",
    "                        },\n",
    "                        popup=folium.GeoJsonPopup(\n",
    "                            fields=[\"popup\"],\n",
    "                            aliases=[\"Info: \"],\n",
    "                            sticky=True,\n",
    "                            max_width=400\n",
    "                        ),\n",
    "                        show=False  # Hide this layer by default\n",
    "                    ).add_to(m)\n",
    "\n",
    "    # Add layer control to toggle between combinations\n",
    "    folium.LayerControl(collapsed=False).add_to(m)\n",
    "\n",
    "    # Add title\n",
    "    title_html = f\"\"\"\n",
    "        <h3 align=\"center\" style=\"font-size:16px\"><b>Region {region_id} Traffic Congestion<br></h3>\n",
    "        \"\"\"\n",
    "    m.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "    return m\n",
    "\n",
    "\n",
    "# Example usage\n",
    "map_region = plot_region_with_filters(region_data=region_data, region_id=1, G=G)\n",
    "map_region.save(\"region_1_traffic_filters_map_new.html\")  # Save as an interactive HTML\n",
    "map_region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.3 Display the shortest routes calculated by inferred travel time and free flow travel time on map for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly pick an hour time\n",
    "hour_time=10\n",
    "region_data_hour=region_data[region_data.index.get_level_values('hour_start')==hour_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to create a directed graph from the region_data dataframe.\n",
    "def create_subset_graph_with_inferred_time(G, region_data_hour):\n",
    "    # Extract relevant edges (u, v) from region_data_hour\n",
    "    relevant_edges = [(row.name[1], row.name[2]) for _, row in region_data_hour.iterrows()]\n",
    "    relevant_nodes = set([node for edge in relevant_edges for node in edge])\n",
    "\n",
    "    # Handle multi-edge graph by selecting the shortest edge for each (u, v)\n",
    "    subset_edges = []\n",
    "    for u, v in relevant_edges:\n",
    "        if G.has_edge(u, v):  \n",
    "            # Find the edge with the minimum 'length' \n",
    "            min_edge_key = min(G[u][v], key=lambda k: G[u][v][k].get('length', float('inf')))\n",
    "            subset_edges.append((u, v, min_edge_key))\n",
    "\n",
    "    # Create the subset graph with the selected edges\n",
    "    G_c = G.edge_subgraph(subset_edges).copy()\n",
    "\n",
    "    # Add the inferred_travel_time_sec attribute from region_data_hour to the edges in G_c\n",
    "    for _, row in region_data_hour.iterrows():\n",
    "        u, v = row.name[1], row.name[2]\n",
    "        edge_keys = list(G_c[u][v].keys())  \n",
    "        first_key = edge_keys[0]  # Get the first key\n",
    "        G_c[u][v][first_key]['inferred_travel_time_sec'] = row['inferred_travel_time_sec']\n",
    "        G_c[u][v][first_key]['congestion_level_explanation'] = row['congestion_level_explanation']\n",
    "        G_c[u][v][first_key]['congestion_ratio'] = row['congestion_ratio']\n",
    "\n",
    "\n",
    "    # Ensure all relevant nodes are included\n",
    "    for node in relevant_nodes:\n",
    "        if node not in G_c.nodes and node in G:\n",
    "            G_c.add_node(node, **G.nodes[node])\n",
    "\n",
    "    return G_c\n",
    "\n",
    "\n",
    "# Example usage\n",
    "G_c = create_subset_graph_with_inferred_time(G, region_data_hour)\n",
    "print(f\"Subset graph has {G_c.number_of_nodes()} nodes and {G_c.number_of_edges()} edges.\")\n",
    "\n",
    "# Example to inspect an edge's attributes\n",
    "u, v, key = next(iter(G_c.edges))\n",
    "print(f\"Edge ({u}, {v}) attributes: {G_c[u][v][key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to find a pair of nodes (start_node, end_node) in the graph with multiple routes between them\n",
    "def find_pair_with_multiple_routes(region_data_hour, G_c):\n",
    "    \n",
    "    # Get unique list of nodes\n",
    "    unique_nodes = list(set(G_c.nodes))  \n",
    "\n",
    "    # Find a pair of nodes with multiple paths\n",
    "    for i, start_node in enumerate(unique_nodes):\n",
    "        for end_node in unique_nodes[i + 1:]:  # Check unique pairs (start_node, end_node)\n",
    "            paths = list(nx.all_simple_paths(G_c, source=start_node, target=end_node, cutoff=5))  # Limit to 5 edges for simplicity\n",
    "            if len(paths) > 1:  # Check if there are multiple paths\n",
    "                return start_node, end_node\n",
    "    # If no suitable pair is found, return None\n",
    "    return None, None\n",
    "# Example usage\n",
    "start_node, end_node = find_pair_with_multiple_routes(region_data_hour, G_c)\n",
    "if start_node and end_node:\n",
    "    print(f\"Found suitable nodes: Start Node = {start_node}, End Node = {end_node}\")\n",
    "else:\n",
    "    print(\"No suitable nodes with multiple routes found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate the shortest route in a graph based on a given weight\n",
    "def calculate_shortest_route(G_c, start_node, end_node, weight):\n",
    "\n",
    "    # Calculate the shortest path\n",
    "    path = nx.shortest_path(G_c, source=start_node, target=end_node, weight=weight)\n",
    "    \n",
    "    # Calculate the total weight of the path\n",
    "    total_time = nx.shortest_path_length(G_c, source=start_node, target=end_node, weight=weight)\n",
    "    \n",
    "    return path, total_time\n",
    "\n",
    "\n",
    "# Shortest route based on real traffic time\n",
    "path_inferred, time_inferred = calculate_shortest_route(G_c, start_node, end_node, weight='inferred_travel_time_sec')\n",
    "print(f\"Shortest path (real traffic): {path_inferred}, Total time: {time_inferred} sec\")\n",
    "\n",
    "# Shortest route based on free-flow travel time\n",
    "path_free_flow, time_free_flow = calculate_shortest_route(G_c, start_node, end_node, weight='travel_time')\n",
    "print(f\"Shortest path (free flow): {path_free_flow}, Total time: {time_free_flow} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to display the calculated shortest routes on a map with detailed segment traffic info\n",
    "def display_shortest_routes_with_details(G_c, start_node, end_node):\n",
    "\n",
    "    # Initialize map centered at the start node\n",
    "    start_coords = (G_c.nodes[start_node]['y'], G_c.nodes[start_node]['x'])  # (latitude, longitude)\n",
    "    m = folium.Map(location=start_coords, zoom_start=13)\n",
    "\n",
    "    # Define weights and their visualization styles\n",
    "    route_styles = {\n",
    "        'inferred_travel_time_sec': {'color': '#E65100', 'dash_array': None},  # Solid red line\n",
    "        'travel_time': {'color': '#E65100', 'dash_array': '5,10'},  # Dashed green line\n",
    "    }\n",
    "\n",
    "    for weight, style in route_styles.items():\n",
    "        # Calculate shortest path and its nodes\n",
    "        path = nx.shortest_path(G_c, source=start_node, target=end_node, weight=weight)\n",
    "        \n",
    "        # Extract edges and their corresponding traffic information\n",
    "        for u, v in zip(path[:-1], path[1:]):  # Iterate over consecutive nodes in the path\n",
    "            edge_data = G_c[u][v]  # Access edge data\n",
    "            edge_keys = list(G_c[u][v].keys())\n",
    "            first_key = edge_keys[0]  # get the first key\n",
    "            edge_geometry = edge_data[first_key].get(\"geometry\")\n",
    "\n",
    "        \n",
    "            # Extract traffic information\n",
    "            inferred_time = edge_data[first_key].get(\"inferred_travel_time_sec\", \"N/A\")\n",
    "            free_flow_time = edge_data[first_key].get(\"travel_time\", \"N/A\")\n",
    "            congestion_level=edge_data[first_key].get(\"congestion_level_explanation\", \"N/A\")\n",
    "            congestion_ratio=edge_data[first_key].get(\"congestion_ratio\", \"N/A\")\n",
    "\n",
    "            # Add the edge as a polyline to the map\n",
    "            line_coords = list(edge_geometry.coords)\n",
    "            line_coords_reversed = [[lat, lon] for lon, lat in line_coords]  # Reverse for folium\n",
    "            folium.PolyLine(\n",
    "                locations=line_coords_reversed,\n",
    "                color=style['color'],\n",
    "                weight=5,\n",
    "                dash_array=style['dash_array'],  # Add dash style for travel_time route\n",
    "                tooltip=f\"<b>Route by {weight}</b><br>\"\n",
    "                        f\"Start: {u}, End: {v}<br>\"\n",
    "                        f\"Inferred Time: {inferred_time} sec<br>\"\n",
    "                        f\"Free Flow Time: {free_flow_time} sec<br> \"\n",
    "                        f\"Congestion_level: {congestion_level}<br>\"\n",
    "                        f\"Congestion_ratio: {congestion_ratio}<br>\"\n",
    "            ).add_to(m)\n",
    "\n",
    "    # Add markers for start and end nodes\n",
    "    folium.Marker(location=start_coords, popup=\"Start Node\", icon=folium.Icon(color=\"blue\")).add_to(m)\n",
    "    end_coords = (G_c.nodes[end_node]['y'], G_c.nodes[end_node]['x'])\n",
    "    folium.Marker(location=end_coords, popup=\"End Node\", icon=folium.Icon(color=\"blue\")).add_to(m)\n",
    "     # Add title\n",
    "    title_html = f\"\"\"\n",
    "        <h3 align=\"center\" style=\"font-size:16px\"><b>Region {region} Traffic Congestion at 10 am<br></h3>\n",
    "        \"\"\"\n",
    "    m.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "\n",
    "    return m\n",
    "\n",
    "\n",
    "# Example usage\n",
    "map_routes = display_shortest_routes_with_details(G_c, start_node, end_node)\n",
    "map_routes.save(\"shortest_routes_with_details_map_new.html\")  # Save as an interactive HTML\n",
    "map_routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5  Build ML models for predicting the  travel time for each road segment cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Check the clustering accuracy by plotting the inferred travel time histogram for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick one region\n",
    "region=1\n",
    "region_withcluster=pd.read_csv(f'region_{region}_withcluster.csv')\n",
    "region_withcluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_withcluster.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data amount within each cluster\n",
    "region_withcluster['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick one cluster for specific analysis\n",
    "cluster=0\n",
    "filtered_cluster=region_withcluster[region_withcluster['cluster']==cluster]\n",
    "filtered_cluster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the duplicated trip_start_timestamps within a cluster\n",
    "duplicated_time=filtered_cluster.duplicated('trip_start_timestamp').sum()\n",
    "# calculate the valid number of rows (unique timestamps) we can use for the modeling\n",
    "unique_timestamps=filtered_cluster.shape[0]-duplicated_time\n",
    "unique_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check the clustering accuracy by plotting the inferred_travel_time_sec histogram for each hour, expecting to see a bell shaped distribution \n",
    "fig, axs=plt.subplots(6, 4, figsize=(30,30))\n",
    "for hour in range(24):\n",
    "    row_ind=hour//4\n",
    "    col_ind=hour%4\n",
    "    ax=axs[row_ind, col_ind]\n",
    "    filtered_cluster_hour=filtered_cluster[filtered_cluster['hour_start']==hour]\n",
    "    filtered_cluster_hour['inferred_travel_time_sec'].plot(ax=ax, kind='hist', bins=100)\n",
    "    ax.set_title(f'Cluster {cluster}', fontsize=10)\n",
    "    ax.set_ylabel('inferred_travel_time_sec_frequency', fontsize=8)\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the overal histogram for all hours\n",
    "filtered_cluster['inferred_travel_time_sec'].plot( kind='hist', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_cluster['inferred_travel_time_sec'].describe().drop('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "- for the hourly histograms, if all of them comply with a bell shape, we can say the clustering works well by grouping road segments with good travel time similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**if no further clustering is needed, check the accuracy  about aggregating the inferred_travel_time_sec by grouping duplicated timestamp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the duplicated trip_start_timestamp \n",
    "filtered_cluster['trip_start_timestamp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly pick a timestamp sample to check the accuracy\n",
    "sample_time='2023-11-11 18:45:00' \n",
    "filtered_cluster[filtered_cluster['trip_start_timestamp']==sample_time]['inferred_travel_time_sec'].plot(kind='hist', bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med=filtered_cluster[filtered_cluster['trip_start_timestamp']==sample_time]['inferred_travel_time_sec'].median()\n",
    "avg=filtered_cluster[filtered_cluster['trip_start_timestamp']==sample_time]['inferred_travel_time_sec'].median()\n",
    "print(f'median: {med}, mean: {avg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "- from the above histogram, we can see the variance of the inferred travel time with the same timestamps is not significant.\n",
    "- the mean and medain are close to each other, and each of them can be a good representative for the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the trip_start_timestamp column to datetime format\n",
    "filtered_cluster['trip_start_timestamp'] = pd.to_datetime(filtered_cluster['trip_start_timestamp'])\n",
    "# add a new column that extracts the day of a month for the following analysis\n",
    "filtered_cluster['day']=filtered_cluster['trip_start_timestamp'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_cluster.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate the rows with the same timestamp\n",
    "filtered_cluster_groupbytime=pd.DataFrame({'inferred_traveltime_sec_updated': \n",
    "                                                 filtered_cluster.groupby('trip_start_timestamp')['inferred_travel_time_sec'].median()})\n",
    "# Set the trip_start_timestamp column as the index\n",
    "filtered_cluster.set_index('trip_start_timestamp', inplace=True)\n",
    "# Sort the DataFrame by the index in ascending order\n",
    "filtered_cluster.sort_index(inplace=True)\n",
    "# update the filtered_cluster dataframe after groupby timestamp so as to remove the columns which are specific to individual road segment since \n",
    "## the aggregation operation made these columns meaningless\n",
    "filtered_cluster_updatedbytime=pd.merge(filtered_cluster_groupbytime, filtered_cluster.loc[:, \n",
    "                                                            ['temp', 'dwpt', 'rhum', 'prcp', 'wdir',  'pres', 'year', 'month','dayname', 'day', 'hour_start']].copy(),\n",
    "                                                            left_index=True, right_index=True)\n",
    "# drop the rows with duplicated timestamp\n",
    "filtered_cluster_updatedbytime=filtered_cluster_updatedbytime[~filtered_cluster_updatedbytime.index.duplicated(keep='first')]\n",
    "filtered_cluster_updatedbytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate rows with the same hour time of the same day since we want to focus on hourly data \n",
    "filtered_cluster_groupbyhour=filtered_cluster_updatedbytime.groupby(['year', 'month', 'day', 'hour_start'])[['inferred_traveltime_sec_updated', 'temp', 'dwpt', 'rhum', 'prcp', 'wdir', 'pres']].median()\n",
    "filtered_cluster_groupbyhour=filtered_cluster_groupbyhour.reset_index()\n",
    "filtered_cluster_updatedbyhour=pd.merge(filtered_cluster_updatedbytime.loc[:, \n",
    "                                                            [ 'year', 'month','day', 'hour_start', 'dayname']].reset_index(), filtered_cluster_groupbyhour,\n",
    "                                                            on=['year', 'month', 'day', 'hour_start'])\n",
    "filtered_cluster_updatedbyhour=filtered_cluster_updatedbyhour.set_index('trip_start_timestamp').drop_duplicates()\n",
    "filtered_cluster_updatedbyhour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**if needs to further divide the cluster into more inner clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column 'morning_afternoon_night' to group hours of day to morning, afternoon, and night time frames \n",
    "filtered_cluster['morning_afternoon_night'] = filtered_cluster['hour_start'].apply(\n",
    "lambda x: 'morning' if 6 <= x < 12 \n",
    "          else 'afternoon' if 12 <= x <18 \n",
    "          else 'night' )\n",
    "# fill the na in hourly aggregation for each segment by their corresponding timeframe aggregated values \n",
    "filtered_cluster_timeframe=filtered_cluster.groupby(['u', 'v', 'morning_afternoon_night'])['inferred_travel_time_sec'].median().unstack()\n",
    "filtered_cluster_hourly=filtered_cluster.groupby(['u', 'v', 'hour_start'])['inferred_travel_time_sec'].median().unstack()\n",
    "filtered_cluster_merge=pd.merge(filtered_cluster_hourly, filtered_cluster_timeframe, left_index=True, right_index=True)\n",
    "filtered_cluster_merge_fillna=filtered_cluster_merge.apply(fill_na, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hourly travel time data\n",
    "hourly_data = filtered_cluster_merge_fillna.loc[:, 0:23].values\n",
    "# Normalize each road type by its total travel time to capture patterns\n",
    "pattern_data = hourly_data / np.sum(hourly_data, axis=1, keepdims=True)\n",
    "# Add the sum (scale) back as a feature\n",
    "scale_feature = np.sum(hourly_data, axis=1).reshape(-1, 1)\n",
    "# Combine pattern and scale features\n",
    "combined_features = np.hstack((pattern_data, scale_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### plot the elbow plot\n",
    "model = KMeans()\n",
    "plot_elbow_curve(model, combined_features, cluster_ranges=range(1, min(10, combined_features.shape[0] + 1)), figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run KMeans for further clustering\n",
    "kmeans = KMeans(n_clusters=4, random_state=42) \n",
    "filtered_cluster_merge_fillna['inner_cluster']=kmeans.fit_predict(combined_features) \n",
    "filtered_cluster_merge_fillna['inner_cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the inner cluster information with the original region dataset\n",
    "filtered_cluster=region_withcluster[region_withcluster['cluster']==cluster]\n",
    "filtered_cluster=pd.merge(filtered_cluster, filtered_cluster_merge_fillna[['inner_cluster']], left_on=['u', 'v'], right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the inner clustering accuracy again\n",
    "inner_cluster=1\n",
    "filtered_inner_cluster=filtered_cluster[filtered_cluster['inner_cluster']==inner_cluster]\n",
    "fig, axs=plt.subplots(6, 4, figsize=(30,30))\n",
    "for hour in range(24):\n",
    "    row_ind=hour//4\n",
    "    col_ind=hour%4\n",
    "    ax=axs[row_ind, col_ind]\n",
    "    filtered_inner_cluster_hour=filtered_inner_cluster[filtered_inner_cluster['hour_start']==hour]\n",
    "    filtered_inner_cluster_hour['inferred_travel_time_sec'].plot(ax=ax, kind='hist', bins=100)\n",
    "    ax.set_title(f'inner_cluster {inner_cluster}', fontsize=10)\n",
    "    ax.set_ylabel('inferred_travel_time_sec_frequency', fontsize=8)\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the inner clustering performance by current K\n",
    "time='hour_start'\n",
    "df_plt=filtered_cluster.groupby(['inner_cluster', time])['inferred_travel_time_sec'].median().unstack().T\n",
    "df_plt['total']=filtered_cluster.groupby(time)['inferred_travel_time_sec'].median()\n",
    "df_plt.iloc[:,:-1].plot(marker='o')\n",
    "df_plt['total'].plot(marker='*', color='black', label='total')\n",
    "plt.ylabel('inferred_travel_time_sec (median)')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 EDA for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=filtered_cluster_updatedbyhour.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the plot of inferred_traveltime_sec_updated vs. timestamp for the whole data:\n",
    "df['inferred_traveltime_sec_updated'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see one month data by randomly picking one month\n",
    "month_sample=5\n",
    "df[df.index.month == month_sample]['inferred_traveltime_sec_updated'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see one week data by randomly picking one week\n",
    "# Filter data for a specific week in May (e.g., May 814)\n",
    "df[\"2023-05-08\":\"2023-05-14\"]['inferred_traveltime_sec_updated'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see one day data by randomly picking one day\n",
    "df['2023-05-01 00:00:00': '2023-05-01 23:00:00']['inferred_traveltime_sec_updated'].plot(marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "- it seems the inferred_travel_time_sec shows a stable trend over time but appears a seasonal pattern with period of 24hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the aggregated inferred_traveltime_sec_updated vs. months\n",
    "df.groupby('month')['inferred_traveltime_sec_updated'].median().plot(kind='line', marker='*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the aggregated inferred_traveltime_sec_updated vs. day of week\n",
    "df.groupby('dayname')['inferred_traveltime_sec_updated'].median().reindex([\n",
    "    'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']).plot(kind='line', marker='*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the aggregated inferred_traveltime_sec_updated vs. hours\n",
    "df.groupby('hour_start')['inferred_traveltime_sec_updated'].median().plot(kind='line', marker='*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "- although we can see some seasonal pattern of the inferred travel time by months and day of week, they are  insignificant by comparing with the pattern by hours.\n",
    "- from the plot, we can divide a day into morning, afternoon, and night timeframes, and in each timeframe, there are two peak_hours as below:\n",
    "morning=[6,7,8,9,10,11]\n",
    "morning_peak=[7,8]\n",
    "afternoon=[12,13,14,15,16,17]\n",
    "afternoon_peak=[16, 17]\n",
    "night=[18,19,20,21,22,23,0,1,2,3,4,5]\n",
    "night_peak=[18,19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the completeness of temporal information\n",
    "month_day_hour=df.groupby(['month','day'])['hour_start'].count().reset_index()\n",
    "for month in range(1, 13):\n",
    "    num_days=len(month_day_hour[month_day_hour['month']==month])\n",
    "    avg_num_hours=round(month_day_hour[month_day_hour['month']==month]['hour_start'].mean(),2)\n",
    "    min_num_hours=month_day_hour[month_day_hour['month']==month]['hour_start'].min()\n",
    "    print(f'month {month}, num_days: {num_days}, avg_num_hours: {avg_num_hours}, min_num_hours: {min_num_hours}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "- It seems the hourly temporal data is incomplete. \n",
    "- we need to fill up the missing values first for an easier operation in the following (like to create the lag features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 fill up missing hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a full range of hours for each day\n",
    "full_hours = (\n",
    "   df.groupby(['year', 'month', 'day'])['hour_start']\n",
    "    .apply(lambda x: pd.Series(np.arange(24)))\n",
    "    .reset_index(level=-1, drop=True)  # Drop the original hours index from apply\n",
    "    .reset_index()\n",
    ")\n",
    "full_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the full_hours with the original dataframe to find missing rows\n",
    "df_fh = pd.merge(full_hours, df, on=['year', 'month', 'day', 'hour_start'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill na for weather related features, use forward-fill or backward-fill directly\n",
    "weather_features=['temp', 'dwpt', 'rhum', 'prcp', 'wdir', 'pres']\n",
    "df_fh[weather_features]=df_fh[weather_features].ffill().bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_cluster_updatedbyhour_full[weather_features].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create timeframe and peak_hours columns to indicate whether the current hour is from morning or afternoon or night and whether it is peak hour\n",
    "df_fh['timeframe']=df_fh['hour_start'].apply(\n",
    "    lambda x: 'morning' if 6<=x<12 else 'afternoon' if 12<=x<18 else 'night')\n",
    "df_fh['peak_hours']=df_fh['hour_start'].apply(lambda x: 1 if x in [7,8,16,17,18,19] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill na for inferred_traveltime_sec_updated based on peak_hours and non_peak_hours in different timeframes\n",
    "# split into peak and non-peak sub datasets for each timeframe\n",
    "morning_peak = df_fh[(df_fh['timeframe']=='morning') & (df_fh['peak_hours']==1)].copy()\n",
    "morning_non_peak = df_fh[(df_fh['timeframe']=='morning') & (df_fh['peak_hours']==0)].copy()\n",
    "\n",
    "afternoon_peak = df_fh[(df_fh['timeframe']=='afternoon') & (df_fh['peak_hours']==1)].copy()\n",
    "afternoon_non_peak = df_fh[(df_fh['timeframe']=='afternoon') & (df_fh['peak_hours']==0)].copy()\n",
    "\n",
    "night_peak = df_fh[(df_fh['timeframe']=='night') & (df_fh['peak_hours']==1)].copy()\n",
    "night_non_peak = df_fh[(df_fh['timeframe']=='night') & (df_fh['peak_hours']==0)].copy()\n",
    "\n",
    "# fill na of inferred_traveltime_sec_updated for peak hours\n",
    "morning_peak['inferred_traveltime_sec_updated'] = morning_peak['inferred_traveltime_sec_updated'].ffill().bfill()\n",
    "afternoon_peak['inferred_traveltime_sec_updated'] = afternoon_peak['inferred_traveltime_sec_updated'].ffill().bfill()\n",
    "night_peak['inferred_traveltime_sec_updated'] = night_peak['inferred_traveltime_sec_updated'].ffill().bfill()\n",
    "# fill na of inferred_traveltime_sec_updated for non-peak hours\n",
    "morning_non_peak['inferred_traveltime_sec_updated'] = morning_non_peak['inferred_traveltime_sec_updated'].ffill().bfill()\n",
    "afternoon_non_peak['inferred_traveltime_sec_updated'] = afternoon_non_peak['inferred_traveltime_sec_updated'].ffill().bfill()\n",
    "night_non_peak['inferred_traveltime_sec_updated'] = night_non_peak['inferred_traveltime_sec_updated'].ffill().bfill()\n",
    "\n",
    "# Combine the data\n",
    "df_fh_filled = pd.concat([morning_peak, morning_non_peak, afternoon_peak, afternoon_non_peak, night_peak, night_non_peak]).sort_index()\n",
    "df_fh_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill na in 'dayname' column based on year, month, and day\n",
    "df_fh_filled['dayname'] = (\n",
    "    df_fh_filled.groupby(['year', 'month', 'day'])['dayname']\n",
    "    .transform(lambda x: x.ffill().bfill())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fh_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_fh_filled.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 Generate lag and rolling features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename 'hour_start' to 'hour'\n",
    "df_fh_filled = df_fh_filled.rename(columns={'hour_start': 'hour'})\n",
    "\n",
    "# create a datetime index\n",
    "df_fh_filled['datetime'] = pd.to_datetime(df_fh_filled[['year', 'month', 'day', 'hour']])\n",
    "\n",
    "# set 'datetime' as the index\n",
    "df_fh_filled = df_fh_filled.set_index('datetime')\n",
    "\n",
    "# sort the dataframe by the new datetime index\n",
    "df_fh_filled = df_fh_filled.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a time series data\n",
    "timeseries = df_fh_filled['inferred_traveltime_sec_updated']\n",
    "\n",
    "# plot ACF and PACF\n",
    "plt.figure(figsize=(15, 9))\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_acf(timeseries, lags=24*60, ax=plt.gca()) # create 60 days autocorrelation\n",
    "plt.title('Autocorrelation')\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_pacf(timeseries, lags=24*3, ax=plt.gca()) # create 3 days partial autocorrelation\n",
    "plt.title('Partial Autocorrelation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "- from the ACF plot, it seems the rolling window of 1 month is good enough since we can clearly see the second month is close to the confidence interval band.\n",
    "- from the PCF plot, it seems lag1, lag2,  lag23, lag24 are good options since all the others are close to the confidence interval band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lag features for inferred_traveltime_sec_updated\n",
    "lags = [1, 2, 23, 24]\n",
    "for lag in lags:\n",
    "    df_fh_filled[f'lag_{lag}'] = df_fh_filled['inferred_traveltime_sec_updated'].shift(lag)\n",
    "df_fh_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define rolling windows in hours\n",
    "windows = {'1w': 7*24, '1m': 30*24}  # Weekly, Monthly (in hours)\n",
    "\n",
    "# create rolling median for each window, also the rolling window only includes the same hour as the current hour\n",
    "for window_name, window_size in windows.items():\n",
    "    # rolling median\n",
    "    df_fh_filled[f'rolling_median_{window_name}'] = (\n",
    "        df_fh_filled.groupby('hour')['inferred_traveltime_sec_updated']\n",
    "        .transform(lambda x: x.rolling(window=window_size, min_periods=1).median())\n",
    "    )\n",
    "    \n",
    "df_fh_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the number of rows with na values is insignificant, we just drop them directly \n",
    "df_fh_filled.dropna(inplace=True)\n",
    "df_fh_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3 EDA on individual features and correlation between features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.3.1 Exploring single numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fh_filled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features=df_fh_filled.select_dtypes(['float64'])\n",
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features.hist(figsize=(16, 20), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the skewness for each feature\n",
    "num_features.skew() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "- it seems the prcp feature is highly skewed, followed by the inferred_traveltime_sec_updated and its lag features.\n",
    "- all weather related features show clear seasonal pattern.\n",
    "- rolling_median_1w and rolling_median_1m have very similar distribution, they might be duplicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.3.2 Exploring the correlation between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the correlation matrix\n",
    "corr = num_features.corr()\n",
    "\n",
    "# plot the heatmap with annotations\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(\n",
    "    corr, \n",
    "    annot=True, \n",
    "    fmt=\".2f\", \n",
    "    cmap=\"coolwarm\", \n",
    "    cbar=True, \n",
    "    annot_kws={\"size\": 10} \n",
    ")\n",
    "plt.title(\"Correlation Matrix with Coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For a better visualization, we need to do some filtering to only display the high correlation values: \n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(\n",
    "    corr[abs(corr)>=0.3], \n",
    "    annot=True, \n",
    "    fmt=\".2f\", \n",
    "    cmap=\"coolwarm\", \n",
    "    cbar=True, \n",
    "    annot_kws={\"size\": 10} \n",
    ")\n",
    "plt.title(\"Correlation Matrix with good coefficients (>=0.3 or <=-0.3)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if we want to see which features are highly correlated with the label, we can do: \n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(\n",
    "    corr[['inferred_traveltime_sec_updated']].iloc[1:], \n",
    "    annot=True, \n",
    "    fmt=\".2f\", \n",
    "    cmap=\"coolwarm\", \n",
    "    cbar=True, \n",
    "    annot_kws={\"size\": 10} \n",
    ")\n",
    "plt.title(\"Correlation between label (inferred travel time) and each other feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis: from all above heatmaps, it seems\n",
    "- the inferred_traveltime_sec_updated has insignificant relationship with all weather related features.\n",
    "- the inferred_traveltime_sec_updated has stronger linear relationship with rolling_median_1w and rolling_median_1m, following by lag_1 and lag_24, then followed by lag_2 and lag_23.\n",
    "- there exists a very strong linear relationship between rolling_median_1w and rolling_median_1m (over 99%), we can consider dropping one of them to avoid duplication.\n",
    "- temp and dwpt also have a strong linear relationship (over 88%), we can consider dropping one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check scatter plots between different features:\n",
    "sns.pairplot(num_features, height=3, aspect=1.5) \n",
    "plt.gcf().set_size_inches(15, 15) \n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "- clearly temp and dwpt, rolling_median_1w and rolling_medain_1m  are linearly correlated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary:\n",
    "- we can drop rolling_median_1w while keepping rolling_median_1m (since rolling_median_1m has more data points to be aggregated).\n",
    "- the features rolling_median_1m, lag_1, lag_24,  lag_2 and lag_23 showed relatively stronger linearly relationship with the target inferred travel time.\n",
    "- temp and dwpt are highly correlated one of which can be dropped.\n",
    "- all weather related feaures showed insignificant correlation with the target inferred travel time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.3.3 Use Xgboost as the base model to do feature engneering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df_fh_filled.copy()\n",
    "#from the above analysis, we can drop rolling_median_1w and temp due to the duplication.\n",
    "data.drop(['rolling_median_1w', 'temp'], axis=1, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the categorical features into 'category' dtype\n",
    "data['dayname'] = data['dayname'].astype('category')\n",
    "data['timeframe'] = data['timeframe'].astype('category')\n",
    "\n",
    "# drop the insignificant features according to the feature importances analysis\n",
    "## we want to drop 'lag_1' and 'lag_2' even they showed significant importances since they are difficult to implement for real-time prediction\n",
    "data.drop(['year', 'month', 'rhum',  'prcp', 'lag_1', 'lag_2'], axis=1, inplace=True)\n",
    "\n",
    "# split data into features and label  \n",
    "X = data.drop('inferred_traveltime_sec_updated', axis=1)\n",
    "y = data['inferred_traveltime_sec_updated']\n",
    "\n",
    "# Train-test split (85% train, 15% test)\n",
    "X_trainfull, X_test, y_trainfull, y_test = train_test_split(X, y, test_size=0.15, random_state=42, shuffle=False)  # shuffle=False for time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-validation split (85% train, 15% validation in trainfull)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainfull, y_trainfull, test_size=0.15, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.003,\n",
    "    max_depth=4,\n",
    "    enable_categorical=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = xgb_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "mae=mean_absolute_error(y_val, y_val_pred)\n",
    "me_ae=median_absolute_error(y_val, y_val_pred)\n",
    "r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}sec\")\n",
    "print(f\"Median Abosolute Error: {me_ae:.2f}sec\")\n",
    "print(f'r2_score: {r2:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "xgb.plot_importance(xgb_model, importance_type='weight')\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regon 1:\n",
    "cluster 0: mean=6sec\n",
    "with original features (only dropped rolling_median_1w and temp):\n",
    "- Mean Absolute Error (MAE): 1.40sec\n",
    "- Median Abosolute Error: 1.15sec\n",
    "- r2_score: 0.18\n",
    "after dropping ['year', 'month', 'rhum',  'pres', 'prcp', 'peak_hours'] features:\n",
    "- Mean Absolute Error (MAE): 1.40sec\n",
    "- Median Abosolute Error: 1.15sec\n",
    "- r2_score: 0.18\n",
    "after further droping ['lag_1', 'lag_2'] which are difficult to implement in reality:\n",
    "- Mean Absolute Error (MAE): 1.42sec\n",
    "- Median Abosolute Error: 1.14sec\n",
    "- r2_score: 0.16\n",
    "cluster 1: mean=38sec\n",
    "with original features (only dropped rolling_median_1w and temp):\n",
    "- Mean Absolute Error (MAE): 4.71sec\n",
    "- Median Abosolute Error: 3.72sec\n",
    "- r2_score: 0.44\n",
    "after ['year',  'rhum',  'pres', 'prcp', 'peak_hours', 'lag_1', 'lag_2' ] features removal:\n",
    "- Mean Absolute Error (MAE): 4.96sec\n",
    "- Median Abosolute Error: 4.07sec\n",
    "- r2_score: 0.41\n",
    "cluster 2: mean=16sec\n",
    "with original features (only dropped rolling_median_1w and temp):\n",
    "- Mean Absolute Error (MAE): 1.84sec\n",
    "- Median Abosolute Error: 1.49sec\n",
    "- r2_score: 0.49\n",
    "after ['year',  'rhum',  'pres', 'prcp', 'peak_hours', 'lag_1', 'lag_2' ] features removal:\n",
    "- Mean Absolute Error (MAE): 1.85sec\n",
    "- Median Abosolute Error: 1.53sec\n",
    "- r2_score: 0.48\n",
    "**cluster 3 (with around 40% missing hours): mean=66sec**\n",
    "with original features (only dropped rolling_median_1w and temp):\n",
    "- Mean Absolute Error (MAE): 10.77sec\n",
    "- Median Abosolute Error: 8.24sec\n",
    "- r2_score: 0.37\n",
    "after ['year',  'rhum',  'pres', 'prcp',  'lag_1', 'lag_2' ] features removal:\n",
    "- Mean Absolute Error (MAE): 11.90sec\n",
    "- Median Abosolute Error: 9.87sec\n",
    "- r2_score: 0.27\n",
    "cluster 4: mean=27sec\n",
    "with original features (only dropped rolling_median_1w):\n",
    "- Mean Absolute Error (MAE): 3.50sec\n",
    "- Median Abosolute Error: 2.98sec\n",
    "- r2_score: 0.42\n",
    "after ['year', 'month',  'rhum',  'prcp',  'lag_1', 'lag_2' ] features removal:\n",
    "- Mean Absolute Error (MAE): 3.60sec\n",
    "- Median Abosolute Error: 3.06sec\n",
    "- r2_score: 0.40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: we can just keep the following features for ML modeling:\n",
    "- rolling_median_1m\n",
    "- dayname\n",
    "- hour\n",
    "- lag_24\n",
    "- lag_23\n",
    "- dwpt\n",
    "- timeframe\n",
    "- day\n",
    "- wdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Machine Learning Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1 Xgboost model with Bayesian optimization (BayesSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define time-series cross-validation\n",
    "n_splits = 5  # number of splits for time-series CV\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# define XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(enable_categorical=True, random_state=42)\n",
    "\n",
    "# define hyperparameter search space\n",
    "param_space = {\n",
    "    'n_estimators': (100, 1000),  # Number of trees\n",
    "    'learning_rate': (0.001, 0.1, 'log-uniform'),  # Learning rate\n",
    "    'max_depth': (3, 10),  # Tree depth\n",
    "    'subsample': (0.6, 1.0, 'uniform'),  # Fraction of samples\n",
    "    'colsample_bytree': (0.6, 1.0, 'uniform')  # Fraction of features\n",
    "}\n",
    "\n",
    "# define custom scoring function (e.g., Mean Absolute Error)\n",
    "scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "# set up Bayesian optimization with cross-validation\n",
    "opt = BayesSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    search_spaces=param_space,\n",
    "    cv=tscv,\n",
    "    n_iter=50,  # Number of iterations for optimization\n",
    "    scoring=scorer,\n",
    "    n_jobs=-1,\n",
    "    verbose=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model with Bayesian optimization\n",
    "opt.fit(X_trainfull, y_trainfull)\n",
    "# Best hyperparameters and model\n",
    "best_params = opt.best_params_\n",
    "best_model = opt.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the cross-validation results specifically for the best estimator\n",
    "# index of the best estimator\n",
    "best_index = opt.best_index_\n",
    "\n",
    "# results for the best estimator\n",
    "best_mean_test_score = -opt.cv_results_['mean_test_score'][best_index]  # Negate to get positive MAE\n",
    "best_std_test_score = opt.cv_results_['std_test_score'][best_index]\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(f\"Best Mean MAE: {best_mean_test_score:.2f} sec\")\n",
    "print(f\"Best Std MAE: {best_std_test_score:.2f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Region 1:\n",
    "- cluster 0:\n",
    "Best Mean MAE: 1.52 sec\n",
    "Best Std MAE: 0.08 sec\n",
    "- cluster 1:\n",
    "Best Mean MAE: 4.87 sec\n",
    "Best Std MAE: 0.18 sec\n",
    "- cluster 2:\n",
    "Best Mean MAE: 1.98 sec\n",
    "Best Std MAE: 0.10 sec\n",
    "- **cluster 3 (more missing hours)**:\n",
    "Best Mean MAE: 12.32 sec\n",
    "Best Std MAE: 0.80 sec\n",
    "- cluster 4:\n",
    "Best Mean MAE: 3.63 sec\n",
    "Best Std MAE: 0.12 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best estimator to a file\n",
    "joblib.dump(best_model, f'best_xgb_model_region{region}_cluster{cluster}.pkl')\n",
    "# Load the model back when needed\n",
    "#loaded_model = joblib.load('best_xgb_model_region1_cluster0.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on a test set\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae = mean_absolute_error(y_test, y_test_pred)\n",
    "me_ae = median_absolute_error(y_test, y_test_pred)\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f} sec\")\n",
    "print(f\"Median Absolute Error: {me_ae:.2f} sec\")\n",
    "print(f\"R2 Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 Prophet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use df instead of data because prophet can automatically handel the missing hours, we can just use the original dataset instead of the one after \n",
    "## filling up the missing hours\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop insignificant features and make uniform hourly spacing\n",
    "df_up=df.drop(['year', 'month', 'day', 'dayname', 'temp', 'rhum', 'prcp', 'pres'], axis=1)\n",
    "df_hourly = df_up.reset_index().resample('h', on='trip_start_timestamp').mean().reset_index()\n",
    "df_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop na after resampling\n",
    "df_hourly.dropna(inplace=True)\n",
    "df_cp=df_hourly.copy()\n",
    "df_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for Prophet\n",
    "df_prophet = df_cp.rename(columns={'trip_start_timestamp': 'ds', 'inferred_traveltime_sec_updated': 'y'})\n",
    "df_prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the external regressors \n",
    "regressors = ['dwpt', 'wdir', 'hour_start']\n",
    "# scale the external regressors\n",
    "\n",
    "# initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit and transform the regressors\n",
    "scaled_regressors = scaler.fit_transform(df_prophet[regressors])\n",
    "\n",
    "# add scaled regressors back to the dataframe\n",
    "df_prophet[['dwpt_scaled', 'wdir_scaled', 'hour_scaled']] = scaled_regressors\n",
    "df_prophet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop useless columns\n",
    "df_prophet.drop(['hour_start', 'dwpt',  'wdir'], axis=1, inplace=True)\n",
    "df_prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the dataframe is ordered by ascending on datetime  \n",
    "df_prophet = df_prophet.sort_values(by=['ds'])\n",
    "df_prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test subsets\n",
    "\n",
    "# define the split point\n",
    "split_point = '2024-02-17 00:00:00'  # 80% for training, 20% for testing\n",
    "\n",
    "# Training and test datasets\n",
    "test = df_prophet[df_prophet['ds']>=split_point]\n",
    "train = df_prophet[df_prophet['ds']<split_point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)/len(df_prophet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform cross-validation\n",
    "\n",
    "# initialize Prophet model\n",
    "model = Prophet()\n",
    "\n",
    "# add scaled external regressors\n",
    "model.add_regressor('dwpt_scaled')\n",
    "model.add_regressor('wdir_scaled')\n",
    "model.add_regressor('hour_scaled')\n",
    "\n",
    "# fit the model to the training dataset\n",
    "model.fit(train)  # for prophet objects initialization\n",
    "\n",
    "# perform cross-validation\n",
    "prophet_cv = cross_validation(\n",
    "    model=model,\n",
    "    initial='150 days',  # initial training period\n",
    "    period='30 days',    # spacing between splits\n",
    "    horizon='7 days'    # forecast horizon for test sets\n",
    ")\n",
    "\n",
    "# view the cross-validation results\n",
    "prophet_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the display option to show all rows\n",
    "#pd.set_option('display.max_rows', None)\n",
    "# get performance metrics\n",
    "prophet_performance = performance_metrics(prophet_cv)\n",
    "prophet_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_performance.set_index('horizon').describe().drop('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Comparison: Prophet vs. XGBoost for Region 1**\n",
    "\n",
    "| **Cluster**   | **Prophet MAE** | **Prophet STD** | **XGBoost Best MAE** | **XGBoost Std of Best MAE** |\n",
    "|---------------|-----------------|-----------------|---------------------------|--------------------------|\n",
    "| **Cluster 0** | 1.52 sec        | 0.14 sec        | 1.52 sec                  | 0.08 sec                 |\n",
    "| **Cluster 1** | 5.09 sec        | 0.71 sec        | 4.87 sec                  | 0.18 sec                 |\n",
    "| **Cluster 2** | 2.13 sec        | 0.21 sec        | 1.98 sec                  | 0.10 sec                 |\n",
    "| **Cluster 3** | **13.30 sec**   | **2.00 sec**    | **12.32 sec**             | **0.80 sec**             |\n",
    "| **Cluster 4** | 3.71 sec        | 0.39 sec        | 3.63 sec                  | 0.12 sec                 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot MAE over horizons\n",
    "plt.figure(figsize=(10, 6))\n",
    "prophet_performance['mae'].plot(marker='o')\n",
    "plt.title('Mean Absolute Error in sec Across Forecast Horizons')\n",
    "plt.xlabel('Horizon (hours)')\n",
    "plt.ylabel('MAE (sec)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the actual values and prediction intervals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(prophet_cv.index, prophet_cv['y'], label='Actual Values', marker='o')\n",
    "plt.fill_between(prophet_cv.index, prophet_cv['yhat_lower'], prophet_cv['yhat_upper'], color='gray', alpha=0.3, label='Confidence Interval')\n",
    "plt.legend()\n",
    "plt.title('Prediction Intervals and Actual Values')\n",
    "plt.xlabel('Observation Index')\n",
    "plt.ylabel('inferred_traveltime_sec_updated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refit the model on the entire training dataset:\n",
    "final_model = Prophet()\n",
    "final_model.fit(train)\n",
    "\n",
    "# predict on the test dataset:\n",
    "future = test\n",
    "forecast = final_model.predict(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merge=pd.merge(test, forecast[['ds', 'yhat']], on=['ds']) \n",
    "# evaluate the model\n",
    "mae=mean_absolute_error(test_merge['y'], test_merge['yhat'])\n",
    "r2 = r2_score(test_merge['y'], test_merge['yhat'])\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}sec\")\n",
    "print(f'r2_score: {r2:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_regions_clusters={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_regions_clusters[f'region_{region}_cluster_{cluster}']=test_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary to a file\n",
    "joblib.dump(test_regions_clusters, f'test_region{region}_clusters.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary from the file\n",
    "test_regions_clusters = joblib.load(f'test_region{region}_clusters.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Use the prediction results to do route optimization and compare the results with that based on the inferred travel time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.1 Merge the prediction results for each cluster to the original regional dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a region dataset to display\n",
    "region=1\n",
    "region_withcluster=pd.read_csv(f'region_{region}_withcluster.csv')\n",
    "region_withcluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_withcluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_withcluster.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 'day' column\n",
    "region_withcluster['trip_start_timestamp']=pd.to_datetime(region_withcluster['trip_start_timestamp'])\n",
    "region_withcluster['day']=region_withcluster['trip_start_timestamp'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_withcluster.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 'datetime' column that has a uniform hourly spacing so as to do merge with the prediction results that with uniform hourly spacing\n",
    "region_withcluster.rename(columns={'hour_start': 'hour'}, inplace=True)\n",
    "region_withcluster['datetime']=pd.to_datetime(region_withcluster[['year', 'month', 'day', 'hour']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby datetime for each road segment to create a dataframe with uique hourly datetime for each segment\n",
    "region_data_groupbytime=region_withcluster.groupby(['u', 'v', 'datetime'])[['speed_kph', 'travel_time',  'inferred_travel_time_sec', \n",
    "                                                                            'year', 'month', 'day', 'hour', 'cluster']].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_data_groupbytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with our prediction results\n",
    "split_point = '2024-02-17 00:00:00'\n",
    "region_data_groupbytime_test=region_data_groupbytime[region_data_groupbytime['datetime']>=split_point].sort_values(by=['datetime'])\n",
    "\n",
    "#remember to update num_clusters for different regions\n",
    "num_clusters=5\n",
    "\n",
    "region_data_ls=[]\n",
    "for cluster in range(num_clusters):\n",
    "    test_regions_clusters[f'region_{region}_cluster_{cluster}']['cluster']=cluster\n",
    "    region_data=pd.merge(region_data_groupbytime_test, test_regions_clusters[f'region_{region}_cluster_{cluster}'][['ds', 'cluster', 'yhat']], \n",
    "                         left_on=['datetime', 'cluster'], right_on=['ds', 'cluster'])\n",
    "    region_data_ls.append(region_data)\n",
    "region_data_merge=pd.concat(region_data_ls)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_data_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.2 Compare the route optimization results by using the predition results on cluster level and the inferred travel time on segment level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.2.1 Analyze the difference between the inferred_travel_time_sec and the predicted travel time for each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze the difference between the inferred_travel_time_sec and the prediction for each segment \n",
    "region_data_merge['abs_diff_sec_actual_pred']=abs(region_data_merge['yhat']-region_data_merge['inferred_travel_time_sec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_data_merge['abs_diff_sec_actual_pred'].plot(kind='hist', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_data_merge['abs_diff_sec_actual_pred'].describe().drop('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.2.2 Compare the route optimization results by the prediction and the inferred travel time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an OSMnx graph for Chicago city\n",
    "place = 'Chicago, Illinois, USA'\n",
    "G = ox.graph_from_place(place, network_type='drive')\n",
    "# create geo-dataframe for both nodes and edges \n",
    "nodes, edges = ox.graph_to_gdfs(G, nodes=True, edges=True)\n",
    "\n",
    "# add geometry from edges to G for plot in the following\n",
    "for idx, row in edges.reset_index().iterrows():\n",
    "    u, v, key = row['u'], row['v'], row['key']\n",
    "    G[u][v][key]['geometry'] = row['geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to create a directed graph based on the given dataset.\n",
    "def create_subset_graph_with_given_data(G, data):\n",
    "    # Extract relevant edges (u, v) from the given data\n",
    "    relevant_edges = [(row['u'], row['v']) for _, row in data.iterrows()]\n",
    "    relevant_nodes = set([node for edge in relevant_edges for node in edge])\n",
    "\n",
    "    # Handle multi-edge graph by selecting the shortest edge for each (u, v)\n",
    "    subset_edges = []\n",
    "    for u, v in relevant_edges:\n",
    "        if G.has_edge(u, v):  \n",
    "            # Find the edge with the minimum 'length' \n",
    "            min_edge_key = min(G[u][v], key=lambda k: G[u][v][k].get('length', float('inf')))\n",
    "            subset_edges.append((u, v, min_edge_key))\n",
    "\n",
    "    # Create the subset graph with the selected edges\n",
    "    G_c = G.edge_subgraph(subset_edges).copy()\n",
    "\n",
    "    # Add the inferred_travel_time_sec and yhat attributes to the edges in G_c\n",
    "    for _, row in data.iterrows():\n",
    "        u, v = row['u'], row['v']\n",
    "        edge_keys = list(G_c[u][v].keys())  \n",
    "        first_key = edge_keys[0]  # Get the first key\n",
    "        G_c[u][v][first_key]['inferred_travel_time_sec'] = row['inferred_travel_time_sec']\n",
    "        G_c[u][v][first_key]['prediction_in_sec'] = row['yhat']\n",
    "     \n",
    "\n",
    "    # Ensure all relevant nodes are included\n",
    "    for node in relevant_nodes:\n",
    "        if node not in G_c.nodes and node in G:\n",
    "            G_c.add_node(node, **G.nodes[node])\n",
    "\n",
    "    return G_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_data_merge['datetime'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a specific time between '2024-02-17 00:00:00' and  '2024-04-30 23:00:00'\n",
    "timestamp='2024-02-23 11:00:00'\n",
    "region_data_merge_specifictime=region_data_merge[region_data_merge['datetime']==timestamp]\n",
    "# create a subgraph corresponding to the specified timestamp\n",
    "G_c = create_subset_graph_with_given_data(G, region_data_merge_specifictime)\n",
    "print(f\"Subset graph has {G_c.number_of_nodes()} nodes and {G_c.number_of_edges()} edges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to inspect an edge's attributes\n",
    "u, v, key = next(iter(G_c.edges))\n",
    "print(f\"Edge ({u}, {v}) attributes: {G_c[u][v][key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to find pairs of nodes (start_node, end_node) with multiple routes\n",
    "def find_pairs_with_multiple_routes(data, G_c, num_pairs=1000):   \n",
    "    # initialize an empty list to store the pairs\n",
    "    node_pairs = []\n",
    "    # get unique list of nodes\n",
    "    unique_nodes = list(set(G_c.nodes))  \n",
    "    # iterate through unique pairs of nodes\n",
    "    for i, start_node in enumerate(unique_nodes):\n",
    "        for end_node in unique_nodes[i + 1:]:  # check unique pairs (start_node, end_node)\n",
    "            # Find all simple paths between the nodes\n",
    "            # I chose 80 as cutoff was based on the fact that the average trip_duration from the existing dataset is 12min, 80 cutoff  similuates this reality\n",
    "            paths = list(nx.all_simple_paths(G_c, source=start_node, target=end_node, cutoff=80))  # Limit to paths of max 80 edges\n",
    "            if len(paths) > 1:  # Check if there are multiple paths\n",
    "                node_pairs.append((start_node, end_node))    \n",
    "            # Stop when we have enough pairs\n",
    "            if len(node_pairs) >= num_pairs:\n",
    "                return node_pairs    \n",
    "    # If less than `num_pairs` are found, return what we have\n",
    "    return node_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_pairs = find_pairs_with_multiple_routes(region_data_merge_specifictime, G_c)\n",
    "len(node_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate the shortest route in a graph based on a given weight\n",
    "def calculate_shortest_route(G_c, start_node, end_node, weight):\n",
    "\n",
    "    # calculate the shortest path\n",
    "    path = nx.shortest_path(G_c, source=start_node, target=end_node, weight=weight)\n",
    "    \n",
    "    # calculate the total weight of the path\n",
    "    total_time = nx.shortest_path_length(G_c, source=start_node, target=end_node, weight=weight)\n",
    "    \n",
    "    return path, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_inferred_ls=[]\n",
    "path_prediction_ls=[]\n",
    "time_inferred_ls=[]\n",
    "#abs_timediff_percent_predtime_inferredtime means the absolute percentage error between the predicted travel time and the actual inferred total travel time. \n",
    "abs_timediff_percent_predtime_inferredtime_ls=[]\n",
    "#abs_timediff_percent__predpath_inferredpath means the absolute percentage difference in travel time between the predicted path and the actual fastest path \n",
    "abs_timediff_percent_predpath_inferredpath_ls=[]\n",
    "#abs_timediff_sec_predpath_inferredpath means the absolute travel time difference in sec between the predicted shortest path and the actual fastest path. \n",
    "abs_timediff_sec_predpath_inferredpath_ls=[]\n",
    "#abs_timediff_sec_predtime_inferredtime means the absolute difference in sec between the predicted travel time and the actual inferred total travel time. \n",
    "abs_timediff_sec_predtime_inferredtime_ls=[]\n",
    "comp_ls=[]\n",
    "for i in range(len(node_pairs)):\n",
    "   # shortest route based on inferred traffic time\n",
    "    path_inferred, time_inferred = calculate_shortest_route(G_c, node_pairs[i][0], node_pairs[i][1], weight='inferred_travel_time_sec')\n",
    "    path_inferred_ls.append(path_inferred)\n",
    "    time_inferred_ls.append(time_inferred)\n",
    "   # shortest route based on predicted travel time\n",
    "    path_prediction, time_prediction = calculate_shortest_route(G_c, node_pairs[i][0], node_pairs[i][1], weight='prediction_in_sec')\n",
    "    path_prediction_ls.append(path_prediction)\n",
    "    comp_ls.append(path_inferred==path_prediction)\n",
    "   # calculate the total inferred travel time for the calculated shortest route based on predicted travel time\n",
    "    inferred_travel_time_fromprediction = 0\n",
    "    for i in range(len(path_prediction) - 1): \n",
    "        u = path_prediction[i]  \n",
    "        v = path_prediction[i + 1]\n",
    "        edge_keys = list(G_c[u][v].keys())  \n",
    "        first_key = edge_keys[0]  \n",
    "        inferred_travel_time_fromprediction += G_c[u][v][first_key]['inferred_travel_time_sec']\n",
    "    abs_timediff_percent_predpath_inferredpath_ls.append(abs(inferred_travel_time_fromprediction-time_inferred)/time_inferred)\n",
    "    abs_timediff_percent_predtime_inferredtime_ls.append(abs(time_prediction-inferred_travel_time_fromprediction)/inferred_travel_time_fromprediction)\n",
    "    abs_timediff_sec_predpath_inferredpath_ls.append(abs(inferred_travel_time_fromprediction-time_inferred))\n",
    "    abs_timediff_sec_predtime_inferredtime_ls.append(abs(time_prediction-inferred_travel_time_fromprediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the average trip_duration from the generated pairs\n",
    "np.mean(time_inferred_ls)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_perf=pd.DataFrame({'absolute percentage time difference between the predicted path and the actual fastest path': \n",
    "                              abs_timediff_percent_predpath_inferredpath_ls, 'absolute percentage error between the predicted travel time and the actual inferred travel time':\n",
    "              abs_timediff_percent_predtime_inferredtime_ls,'absolute time difference in sec between the predicted path and the actual fastest path': \n",
    "                              abs_timediff_sec_predpath_inferredpath_ls, 'absolute time difference in sec between the predicted travel time and the actual inferred travel time':\n",
    "              abs_timediff_sec_predtime_inferredtime_ls})\n",
    "prediction_perf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_perf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of the shortest route calculated by prediction matches with that by inferred travel time\n",
    "len(list(filter(lambda x: True if x is True else False, comp_ls)))/len(comp_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to display the calculated shortest routes on a map with traffic info\n",
    "def display_shortest_routes_with_traveltime_info(G_c, start_node, end_node):\n",
    "\n",
    "    # Initialize map centered at the start node\n",
    "    start_coords = (G_c.nodes[start_node]['y'], G_c.nodes[start_node]['x'])  # (latitude, longitude)\n",
    "    m = folium.Map(location=start_coords, zoom_start=13)\n",
    "\n",
    "    # Define weights and their visualization styles\n",
    "    route_styles = {\n",
    "        'inferred_travel_time_sec': {'color': '#E65100', 'dash_array': None},  # Solid line\n",
    "        'prediction_in_sec': {'color': '#E65100', 'dash_array': '5,10'},  # Dashed line\n",
    "    }\n",
    "\n",
    "    total_times = {}  # Dictionary to store total travel times for each weight\n",
    "\n",
    "    for weight, style in route_styles.items():\n",
    "        # calculate shortest path and its nodes\n",
    "        path = nx.shortest_path(G_c, source=start_node, target=end_node, weight=weight)\n",
    "        \n",
    "        # initialize total travel time\n",
    "        total_time = 0\n",
    "\n",
    "        # extract edges and their corresponding traffic information\n",
    "        for u, v in zip(path[:-1], path[1:]):  # iterate over consecutive nodes in the path\n",
    "            edge_data = G_c[u][v]  # access edge data\n",
    "            edge_keys = list(G_c[u][v].keys())\n",
    "            first_key = edge_keys[0]  # get the first key\n",
    "            edge_geometry = edge_data[first_key].get(\"geometry\")\n",
    "\n",
    "            # extract traffic information\n",
    "            inferred_time = edge_data[first_key].get(\"inferred_travel_time_sec\", 0)\n",
    "            prediction_time = edge_data[first_key].get(\"prediction_in_sec\", 0)\n",
    "\n",
    "            # add time to the total for the current weight\n",
    "            travel_time = edge_data[first_key].get(weight, 0)  \n",
    "            total_time += travel_time\n",
    "\n",
    "            # add the edge as a polyline to the map\n",
    " \n",
    "            line_coords = list(edge_geometry.coords)\n",
    "            line_coords_reversed = [[lat, lon] for lon, lat in line_coords]  # Reverse for folium\n",
    "            folium.PolyLine(\n",
    "                locations=line_coords_reversed,\n",
    "                color=style['color'],\n",
    "                weight=5,\n",
    "                dash_array=style['dash_array'],\n",
    "                tooltip=f\"<b>Route by {weight}</b><br>\"\n",
    "                            f\"Start: {u}, End: {v}<br>\"\n",
    "                            f\"Inferred Time: {inferred_time} sec<br>\"\n",
    "                            f\"Predicted Time: {prediction_time} sec<br>\"\n",
    "                ).add_to(m)\n",
    "\n",
    "        # store the total travel time in minutes\n",
    "        total_times[weight] = total_time / 60  # convert seconds to minutes\n",
    "\n",
    "    # add markers for start and end nodes\n",
    "    folium.Marker(location=start_coords, popup=\"Start Node\", icon=folium.Icon(color=\"blue\")).add_to(m)\n",
    "    end_coords = (G_c.nodes[end_node]['y'], G_c.nodes[end_node]['x'])\n",
    "\n",
    "    # add total travel time popup at the End Node\n",
    "    total_time_popup = \"<br>\".join(\n",
    "        [f\"<b>Route by {weight}</b>: {total_time:.2f} min\" for weight, total_time in total_times.items()]\n",
    "    )\n",
    "    folium.Marker(\n",
    "        location=end_coords,\n",
    "        popup=f\"<b>End Node</b><br>{total_time_popup}\",\n",
    "        icon=folium.Icon(color=\"blue\")\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Add title\n",
    "    title_html = f\"\"\"\n",
    "        <h3 align=\"center\" style=\"font-size:16px\"><b>Region {region} Route Optimization Comparison at 2024-02-23 11:00:00 am<br></h3>\n",
    "        \"\"\"\n",
    "    m.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "    return m\n",
    "\n",
    "\n",
    "# Example usage\n",
    "map_routes = display_shortest_routes_with_traveltime_info(G_c, node_pairs[2][0], node_pairs[2][1])\n",
    "map_routes.save(\"shortest_routes_comparison.html\")  # Save as an interactive HTML\n",
    "map_routes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
